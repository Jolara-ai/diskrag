#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æº–å‚™SIFTæ•¸æ“šé›†ç‚ºDiskRAG collectionæ ¼å¼
"""

import sys
import os
import numpy as np
import pandas as pd
import json
from pathlib import Path
from typing import Dict, Any
import argparse

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from preprocessing.collection import CollectionManager
from preprocessing.config import CollectionInfo

def load_sift_vectors(parquet_path: str) -> np.ndarray:
    """
    å¾parquetæ–‡ä»¶è¼‰å…¥SIFTå‘é‡
    
    Args:
        parquet_path: parquetæ–‡ä»¶è·¯å¾‘
    
    Returns:
        å‘é‡æ•¸çµ„ (N, 128)
    """
    print(f"è¼‰å…¥SIFTå‘é‡: {parquet_path}")
    df = pd.read_parquet(parquet_path)
    print(f"æ•¸æ“šå½¢ç‹€: {df.shape}")
    print(f"åˆ—å: {df.columns.tolist()}")
    
    # å˜—è©¦ä¸åŒçš„åˆ—åä¾†æ‰¾åˆ°å‘é‡æ•¸æ“š
    if 'vector' in df.columns:
        vectors = np.stack(df['vector'].values)
    elif 'embedding' in df.columns:
        vectors = np.stack(df['embedding'].values)
    elif 'features' in df.columns:
        vectors = np.stack(df['features'].values)
    else:
        # å˜—è©¦æ‰¾åˆ°åŒ…å«æ•¸å€¼æ•¸æ“šçš„åˆ—
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 128:
            vectors = df[numeric_cols].values.astype(np.float32)
        else:
            # å¦‚æœæ²’æœ‰128åˆ—ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•
            print(f"è­¦å‘Š: æ²’æœ‰æ‰¾åˆ°128ç¶­å‘é‡æ•¸æ“š")
            print(f"æ•¸å€¼åˆ—æ•¸é‡: {len(numeric_cols)}")
            print(f"æ•¸å€¼åˆ—: {numeric_cols.tolist()}")
            
            # å˜—è©¦å°‡æ‰€æœ‰æ•¸å€¼åˆ—ä½œç‚ºå‘é‡
            if len(numeric_cols) > 0:
                vectors = df[numeric_cols].values.astype(np.float32)
                print(f"ä½¿ç”¨æ‰€æœ‰æ•¸å€¼åˆ—ä½œç‚ºå‘é‡ï¼Œç¶­åº¦: {vectors.shape[1]}")
            else:
                raise ValueError("ç„¡æ³•æ‰¾åˆ°å‘é‡æ•¸æ“š")
    
    print(f"å‘é‡å½¢ç‹€: {vectors.shape}")
    print(f"å‘é‡é¡å‹: {vectors.dtype}")
    print(f"å‘é‡ç¯„åœ: [{np.min(vectors):.6f}, {np.max(vectors):.6f}]")
    
    return vectors

def create_sift_collection(
    collection_name: str,
    train_path: str,
    test_path: str = None,
    max_train_samples: int = None,
    max_test_samples: int = None
) -> bool:
    """
    å‰µå»ºSIFT collection
    
    Args:
        collection_name: collectionåç¨±
        train_path: è¨“ç·´æ•¸æ“šè·¯å¾‘
        test_path: æ¸¬è©¦æ•¸æ“šè·¯å¾‘ï¼ˆå¯é¸ï¼‰
        max_train_samples: æœ€å¤§è¨“ç·´æ¨£æœ¬æ•¸
        max_test_samples: æœ€å¤§æ¸¬è©¦æ¨£æœ¬æ•¸
    
    Returns:
        æ˜¯å¦æˆåŠŸå‰µå»º
    """
    print(f"å‰µå»ºSIFT collection: {collection_name}")
    
    # è¼‰å…¥è¨“ç·´æ•¸æ“š
    train_vectors = load_sift_vectors(train_path)
    
    # é™åˆ¶è¨“ç·´æ¨£æœ¬æ•¸
    if max_train_samples and len(train_vectors) > max_train_samples:
        print(f"é™åˆ¶è¨“ç·´æ¨£æœ¬æ•¸ç‚º {max_train_samples}")
        indices = np.random.choice(len(train_vectors), max_train_samples, replace=False)
        train_vectors = train_vectors[indices]
    
    # å‰µå»ºcollection manager
    manager = CollectionManager()
    
    # æª¢æŸ¥collectionæ˜¯å¦å·²å­˜åœ¨
    if manager.get_collection_info(collection_name):
        print(f"Collection '{collection_name}' å·²å­˜åœ¨")
        response = input("æ˜¯å¦è¦è¦†è“‹ï¼Ÿ(y/N): ")
        if response.lower() != 'y':
            print("å–æ¶ˆå‰µå»º")
            return False
    
    # å‰µå»ºcollectionç›®éŒ„
    collection_dir = manager._get_collection_dir(collection_name)
    collection_dir.mkdir(parents=True, exist_ok=True)
    
    # å‰µå»ºcollection
    print(f"å‰µå»ºcollection: {collection_name}")
    collection_info = CollectionInfo(
        name=collection_name,
        dimension=train_vectors.shape[1],
        num_vectors=len(train_vectors),
        created_at=pd.Timestamp.now().isoformat(),
        updated_at=pd.Timestamp.now().isoformat(),
        source_files=[train_path],
        config={
            "description": f"SIFTæ•¸æ“šé›† - {len(train_vectors):,}å€‹å‘é‡",
            "type": "sift_dataset"
        },
        chunk_stats={
            "total_chunks": len(train_vectors),
            "avg_chunk_size": train_vectors.shape[1],
            "created_at": pd.Timestamp.now().isoformat()
        }
    )
    
    # ä¿å­˜collectionä¿¡æ¯
    manager.save_collection_info(collection_name, collection_info)
    
    # ä¿å­˜å‘é‡æ•¸æ“š
    vectors_path = manager.get_vectors_path(collection_name)
    print(f"ä¿å­˜å‘é‡åˆ°: {vectors_path}")
    np.save(str(vectors_path), train_vectors.astype(np.float32))
    
    # å‰µå»ºmetadata
    metadata = []
    for i in range(len(train_vectors)):
        metadata.append({
            "id": i,
            "text": f"SIFT_vector_{i:06d}",
            "text_hash": f"sift_{i:06d}",
            "chunk_id": i,
            "chunk_index": 0,
            "embedding": train_vectors[i].tolist()
        })
    
    # ä¿å­˜metadata
    metadata_path = manager.get_metadata_path(collection_name)
    print(f"ä¿å­˜metadataåˆ°: {metadata_path}")
    metadata_df = pd.DataFrame(metadata)
    metadata_df.to_parquet(str(metadata_path), index=False)
    
    print(f"âœ… SIFT collectionå‰µå»ºæˆåŠŸ")
    print(f"  åç¨±: {collection_name}")
    print(f"  å‘é‡æ•¸é‡: {len(train_vectors):,}")
    print(f"  å‘é‡ç¶­åº¦: {train_vectors.shape[1]}")
    
    # å¦‚æœæä¾›äº†æ¸¬è©¦æ•¸æ“šï¼Œä¹Ÿä¿å­˜æ¸¬è©¦å‘é‡
    if test_path:
        print(f"\nè™•ç†æ¸¬è©¦æ•¸æ“š...")
        test_vectors = load_sift_vectors(test_path)
        
        if max_test_samples and len(test_vectors) > max_test_samples:
            print(f"é™åˆ¶æ¸¬è©¦æ¨£æœ¬æ•¸ç‚º {max_test_samples}")
            indices = np.random.choice(len(test_vectors), max_test_samples, replace=False)
            test_vectors = test_vectors[indices]
        
        # ä¿å­˜æ¸¬è©¦å‘é‡åˆ°å–®ç¨çš„æ–‡ä»¶
        test_vectors_path = Path(f"collections/{collection_name}/test_vectors.npy")
        test_vectors_path.parent.mkdir(parents=True, exist_ok=True)
        np.save(str(test_vectors_path), test_vectors.astype(np.float32))
        
        print(f"æ¸¬è©¦å‘é‡ä¿å­˜åˆ°: {test_vectors_path}")
        print(f"æ¸¬è©¦å‘é‡æ•¸é‡: {len(test_vectors):,}")
    
    return True

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='æº–å‚™SIFTæ•¸æ“šé›†ç‚ºDiskRAG collection')
    parser.add_argument('--collection-name', type=str, default='sift500k',
                       help='collectionåç¨±')
    parser.add_argument('--train-path', type=str,
                       default='dataset/sift_small_500k/train_fixed.parquet',
                       help='è¨“ç·´æ•¸æ“šè·¯å¾‘')
    parser.add_argument('--test-path', type=str,
                       default='dataset/sift_small_500k/test_fixed.parquet',
                       help='æ¸¬è©¦æ•¸æ“šè·¯å¾‘')
    parser.add_argument('--max-train-samples', type=int, default=None,
                       help='æœ€å¤§è¨“ç·´æ¨£æœ¬æ•¸')
    parser.add_argument('--max-test-samples', type=int, default=10000,
                       help='æœ€å¤§æ¸¬è©¦æ¨£æœ¬æ•¸')
    
    args = parser.parse_args()
    
    # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.train_path):
        print(f"âŒ è¨“ç·´æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {args.train_path}")
        return 1
    
    if args.test_path and not os.path.exists(args.test_path):
        print(f"âŒ æ¸¬è©¦æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {args.test_path}")
        return 1
    
    # å‰µå»ºcollection
    success = create_sift_collection(
        collection_name=args.collection_name,
        train_path=args.train_path,
        test_path=args.test_path,
        max_train_samples=args.max_train_samples,
        max_test_samples=args.max_test_samples
    )
    
    if success:
        print("\nğŸ‰ SIFT collectionæº–å‚™å®Œæˆï¼")
        print(f"ç¾åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å»ºç½®ç´¢å¼•:")
        print(f"python build_index.py {args.collection_name}")
        return 0
    else:
        print("\nâŒ SIFT collectionæº–å‚™å¤±æ•—")
        return 1

if __name__ == "__main__":
    exit(main()) 