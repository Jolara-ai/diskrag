#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DiskRAG Academic Validation on GIST-1M Dataset
å­¸è¡“ç´š GIST-1M æ•¸æ“šé›†é©—è­‰è…³æœ¬

ä½¿ç”¨æ–¹æ³•:
1. è§£å£“ gist.tar.gz åˆ° data/gist/ ç›®éŒ„
2. python gist_academic_validation.py --scale small_scale
3. æŸ¥çœ‹ç”Ÿæˆçš„å­¸è¡“å ±å‘Š
"""

import numpy as np
import argparse
import logging
import time
import json
from pathlib import Path
import sys
import os
from tqdm import tqdm

# å°å…¥æ‚¨çš„æ¨¡çµ„
try:
    from pydiskann.pq.fast_pq import DiskANNPQ
    from pydiskann.vamana_graph import build_vamana
    from pydiskann.io.diskann_persist import DiskANNPersist
    from search_engine import SearchEngine
    from preprocessing.collection import CollectionManager
except ImportError as e:
    print(f"éŒ¯èª¤: ç„¡æ³•å°å…¥æ¨¡çµ„: {e}")
    sys.exit(1)

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("è­¦å‘Š: æœªå®‰è£ Faissï¼Œå°‡ç„¡æ³•è¨ˆç®—å‹•æ…‹ Ground Truth")

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("GISTAcademicValidation")

# å­¸è¡“è©•ä¼°æ¨™æº–é…ç½®
ACADEMIC_SCALES = {
    "small_scale": {
        "base_size": 100000,
        "learn_size": 50000,
        "query_size": 1000,
        "description": "100K åŸºç¤å‘é‡é©—è­‰"
    },
    "medium_scale": {
        "base_size": 300000,
        "learn_size": 100000,
        "query_size": 1000,
        "description": "300K åŸºç¤å‘é‡é©—è­‰"
    },
    "large_scale": {
        "base_size": 500000,
        "learn_size": 150000,
        "query_size": 1000,
        "description": "500K åŸºç¤å‘é‡é©—è­‰"
    },
    "full_scale": {
        "base_size": 1000000,
        "learn_size": 500000,
        "query_size": 1000,
        "description": "å®Œæ•´ 1M åŸºç¤å‘é‡é©—è­‰"
    }
}

# DiskANN è«–æ–‡æ¨™æº–é…ç½®
DISKANN_CONFIGS = {
    "baseline": {
        "R": 32,
        "L_build": 64,
        "pq_m": 16,
        "description": "åŸºç¤é…ç½®"
    },
    "balanced": {
        "R": 48,
        "L_build": 100,
        "pq_m": 24,
        "description": "å¹³è¡¡é…ç½®ï¼ˆæ¨è–¦ï¼‰"
    },
    "high_recall": {
        "R": 64,
        "L_build": 128,
        "pq_m": 32,
        "description": "é«˜å¬å›ç‡é…ç½®"
    }
}

# å­¸è¡“è©•ä¼°æ¨™æº– L_search ç¯„åœ
ACADEMIC_L_SEARCH = [10, 20, 50, 100, 200, 500, 800, 1000, 1500, 2000, 3000,5000,10000]

def read_fvecs(filename):
    """è®€å– .fvecs æ ¼å¼æ–‡ä»¶"""
    with open(filename, 'rb') as f:
        fv = np.fromfile(f, dtype=np.float32)
    if fv.size == 0:
        return np.zeros((0, 0))
    dim = fv.view(np.int32)[0]
    fv = fv.reshape(-1, dim + 1)
    return fv[:, 1:].copy()

def read_ivecs(filename):
    """è®€å– .ivecs æ ¼å¼æ–‡ä»¶"""
    with open(filename, 'rb') as f:
        iv = np.fromfile(f, dtype=np.int32)
    if iv.size == 0:
        return np.zeros((0, 0))
    dim = iv[0]
    iv = iv.reshape(-1, dim + 1)
    return iv[:, 1:]

def verify_gist_data(data_dir):
    """é©—è­‰ GIST æ•¸æ“šé›†å®Œæ•´æ€§"""
    data_path = Path(data_dir)
    
    required_files = {
        "gist_base.fvecs": (1000000, 960),
        "gist_learn.fvecs": (500000, 960),
        "gist_query.fvecs": (1000, 960),
        "gist_groundtruth.ivecs": (1000, 100)
    }
    
    logger.info("ğŸ” é©—è­‰ GIST æ•¸æ“šé›†...")
    
    for filename, (expected_count, expected_dim) in required_files.items():
        filepath = data_path / filename
        if not filepath.exists():
            logger.error(f"âŒ ç¼ºå°‘æ–‡ä»¶: {filename}")
            return False
            
        try:
            if filename.endswith('.fvecs'):
                data = read_fvecs(str(filepath))
            else:
                data = read_ivecs(str(filepath))
                
            actual_count, actual_dim = data.shape
            logger.info(f"âœ… {filename}: {actual_count} Ã— {actual_dim}")
            
            if actual_count != expected_count:
                logger.warning(f"âš ï¸  {filename} æ•¸é‡: é æœŸ {expected_count}, å¯¦éš› {actual_count}")
            if actual_dim != expected_dim:
                logger.warning(f"âš ï¸  {filename} ç¶­åº¦: é æœŸ {expected_dim}, å¯¦éš› {actual_dim}")
                
        except Exception as e:
            logger.error(f"âŒ è®€å– {filename} å¤±æ•—: {e}")
            return False
    
    logger.info("âœ… GIST æ•¸æ“šé›†é©—è­‰å®Œæˆ")
    return True

def compute_ground_truth(base_vectors, query_vectors, k=100):
    """è¨ˆç®—ç²¾ç¢ºçš„ Ground Truth"""
    if not FAISS_AVAILABLE:
        raise ImportError("éœ€è¦å®‰è£ Faiss: pip install faiss-cpu")
    
    logger.info(f"ğŸ§¬ è¨ˆç®— Ground Truth (k={k})...")
    d = base_vectors.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(base_vectors.astype(np.float32))
    _, gt_indices = index.search(query_vectors.astype(np.float32), k)
    logger.info("âœ… Ground Truth è¨ˆç®—å®Œæˆ")
    return gt_indices

def build_academic_index(collection_name, base_vectors, learn_vectors, config):
    """å»ºç«‹å­¸è¡“é©—è­‰ç´¢å¼•"""
    logger.info(f"ğŸ”§ å»ºç«‹å­¸è¡“ç´¢å¼•: {collection_name}")
    
    base_vectors = base_vectors.astype(np.float32)
    learn_vectors = learn_vectors.astype(np.float32)
    
    build_start_time = time.time()
    n_points, dimension = base_vectors.shape
    
    # ç®¡ç† collection
    manager = CollectionManager()
    if manager.get_collection_info(collection_name):
        logger.warning(f"âš ï¸  Collection '{collection_name}' å·²å­˜åœ¨ï¼Œå°‡è¢«è¦†è“‹")
        manager.delete_collection(collection_name)
    
    manager.create_collection(
        collection_name=collection_name,
        config={},
        dimension=dimension,
        source_files=[]
    )
    index_dir = manager.get_index_dir(collection_name)
    os.makedirs(index_dir, exist_ok=True)
    
    # è¨“ç·´ PQ æ¨¡å‹
    logger.info(f"ğŸ“š è¨“ç·´ PQ æ¨¡å‹ (m={config['pq_m']})...")
    pq_start_time = time.time()
    pq_model = DiskANNPQ(n_subvectors=config['pq_m'])
    pq_model.fit(learn_vectors, show_progress=True)
    pq_train_time = time.time() - pq_start_time
    
    # ç·¨ç¢¼å‘é‡
    logger.info("ğŸ”¢ PQ ç·¨ç¢¼...")
    encode_start_time = time.time()
    pq_codes = pq_model.encode(base_vectors)
    pq_encode_time = time.time() - encode_start_time
    
    # å»ºç«‹åœ–
    logger.info(f"ğŸ•¸ï¸  å»ºç«‹ Vamana åœ– (R={config['R']}, L={config['L_build']})...")
    graph_start_time = time.time()
    graph = build_vamana(base_vectors, R=config['R'], L=config['L_build'], 
                        alpha=1.2, show_progress=True)
    graph_build_time = time.time() - graph_start_time
    
    # ä¿å­˜ç´¢å¼•
    logger.info("ğŸ’¾ ä¿å­˜ç´¢å¼•...")
    persist_start_time = time.time()
    persist = DiskANNPersist(dim=dimension, R=config['R'])
    persist.save_pq_codebook(str(index_dir / "pq_model.pkl"), pq_model)
    persist.save_pq_codes(str(index_dir / "pq_codes.bin"), pq_codes)
    persist.save_index(str(index_dir / "index.dat"), graph)
    
    medoid_idx = getattr(graph, 'medoid_idx', 0)
    meta_info = {
        "D": int(dimension), "R": int(config['R']), "L": int(config['L_build']),
        "alpha": 1.2, "N": int(n_points), "medoid_idx": int(medoid_idx),
        "n_subvectors": int(config['pq_m']), "pq_centroids": 256
    }
    persist.save_meta(str(index_dir / "meta.json"), meta_info)
    
    # å‰µå»ºå…ƒæ•¸æ“šï¼ˆå­¸è¡“é©—è­‰ç”¨ï¼‰
    import polars as pl
    metadata_df = pl.DataFrame({
        "text": [f"gist_vector_{i}" for i in range(n_points)],
        "text_hash": [f"gist_hash_{i}" for i in range(n_points)],
        "metadata": [json.dumps({"id": i, "type": "gist_feature"}) for i in range(n_points)],
        "vector_index": list(range(n_points))
    })
    metadata_df.write_parquet(manager.get_metadata_path(collection_name))
    
    # æ›´æ–° collection info
    info = manager.get_collection_info(collection_name)
    info.num_vectors = n_points
    info.updated_at = time.strftime('%Y-%m-%dT%H:%M:%S')
    manager.save_collection_info(collection_name, info)
    
    persist_time = time.time() - persist_start_time
    total_build_time = time.time() - build_start_time
    
    build_stats = {
        "pq_train_time": pq_train_time,
        "pq_encode_time": pq_encode_time,
        "graph_build_time": graph_build_time,
        "persist_time": persist_time,
        "total_build_time": total_build_time
    }
    
    logger.info(f"âœ… ç´¢å¼•å»ºç«‹å®Œæˆ (ç¸½è€—æ™‚: {total_build_time:.2f}s)")
    return build_stats

def evaluate_academic_performance(collection_name, query_vectors, ground_truth, k=10):
    """åŸ·è¡Œå­¸è¡“ç´šæ€§èƒ½è©•ä¼°"""
    logger.info(f"ğŸ” é–‹å§‹å­¸è¡“ç´šæ€§èƒ½è©•ä¼°...")
    
    engine = SearchEngine(collection_name)
    results = []
    
    for L_search in tqdm(ACADEMIC_L_SEARCH, desc="è©•ä¼°ä¸åŒ L_search å€¼"):
        if L_search < k:
            continue
            
        logger.info(f"  è©•ä¼° L_search={L_search}...")
        
        # é ç†±
        warmup_queries = min(50, len(query_vectors))
        for i in range(warmup_queries):
            engine.search(
                query="", k=k, 
                embedding_fn=lambda x: query_vectors[i],
                L_search=L_search, use_pq_search=True
            )
        
        # æ­£å¼è©•ä¼°
        latencies = []
        total_recall = 0.0
        
        for i in range(len(query_vectors)):
            query_vector = query_vectors[i]
            gt_ids = set(ground_truth[i, :k])
            
            start_time = time.perf_counter()
            search_results = engine.search(
                query="", k=k,
                embedding_fn=lambda x: query_vector,
                L_search=L_search, use_pq_search=True
            )
            latencies.append(time.perf_counter() - start_time)
            
            # æå–è¿”å›çš„ ID
            returned_ids = {
                res["metadata"]["id"] 
                for res in search_results.get("results", []) 
                if "id" in res.get("metadata", {})
            }
            
            # è¨ˆç®— recall
            recall = len(gt_ids.intersection(returned_ids)) / k
            total_recall += recall
        
        avg_recall = total_recall / len(query_vectors)
        avg_latency_ms = (sum(latencies) / len(latencies)) * 1000
        qps = 1000 / avg_latency_ms
        
        result = {
            "L_search": L_search,
            "recall": avg_recall,
            "latency_ms": avg_latency_ms,
            "qps": qps
        }
        
        results.append(result)
        logger.info(f"    Recall: {avg_recall:.4f}, Latency: {avg_latency_ms:.2f}ms, QPS: {qps:.1f}")
    
    return results

def generate_academic_report(scale_config, diskann_config, build_stats, eval_results, 
                           dataset_info):
    """ç”Ÿæˆå­¸è¡“é©—è­‰å ±å‘Š"""
    
    # è¨ˆç®—é—œéµå­¸è¡“æŒ‡æ¨™
    academic_metrics = {}
    
    # æ‰¾åˆ°é”åˆ°ç‰¹å®š recall é–¾å€¼çš„æœ€å° L
    for target_recall in [0.8, 0.85, 0.9, 0.95]:
        for result in eval_results:
            if result["recall"] >= target_recall:
                key = f"recall_{int(target_recall*100)}"
                if key not in academic_metrics:
                    academic_metrics[key] = {
                        "min_L_search": result["L_search"],
                        "actual_recall": result["recall"],
                        "latency_ms": result["latency_ms"],
                        "qps": result["qps"]
                    }
                break
    
    # æœ€ä½³æ€§èƒ½
    best_result = max(eval_results, key=lambda x: x["recall"])
    academic_metrics["best_performance"] = {
        "max_recall": best_result["recall"],
        "L_search": best_result["L_search"],
        "latency_ms": best_result["latency_ms"],
        "qps": best_result["qps"]
    }
    
    # ç”Ÿæˆå®Œæ•´å ±å‘Š
    report = {
        "experiment_metadata": {
            "dataset": "GIST-1M",
            "system": "DiskRAG (DiskANN + PQ Implementation)",
            "evaluation_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "scale": scale_config["description"],
            "configuration": diskann_config["description"]
        },
        "dataset_statistics": dataset_info,
        "index_parameters": {
            "R": diskann_config["R"],
            "L_build": diskann_config["L_build"],
            "pq_subvectors": diskann_config["pq_m"],
            "pq_centroids": 256,
            "alpha": 1.2
        },
        "build_performance": build_stats,
        "search_performance": eval_results,
        "academic_metrics": academic_metrics,
        "diskann_paper_comparison": {
            "note": "èˆ‡ DiskANN åŸè«–æ–‡åœ¨ GIST-1M ä¸Šçš„å°æ¯”",
            "paper_recall_90": "~1000 L_search",
            "paper_recall_95": "~2000 L_search",
            "your_recall_90": academic_metrics.get("recall_90", {}).get("min_L_search", "N/A"),
            "your_recall_95": academic_metrics.get("recall_95", {}).get("min_L_search", "N/A")
        }
    }
    
    return report

def print_academic_summary(report):
    """æ‰“å°å­¸è¡“é©—è­‰æ‘˜è¦"""
    print("\n" + "="*80)
    print("ğŸ† DiskRAG å­¸è¡“é©—è­‰å ±å‘Šæ‘˜è¦")
    print("="*80)
    
    print(f"ğŸ“Š æ•¸æ“šé›†: {report['experiment_metadata']['dataset']}")
    print(f"ğŸ“ è¦æ¨¡: {report['experiment_metadata']['scale']}")
    print(f"âš™ï¸  é…ç½®: {report['experiment_metadata']['configuration']}")
    
    print(f"\nğŸ“ˆ é—œéµæ€§èƒ½æŒ‡æ¨™:")
    metrics = report["academic_metrics"]
    
    best = metrics["best_performance"]
    print(f"  ğŸ¥‡ æœ€ä½³ Recall: {best['max_recall']:.4f} @ L={best['L_search']}")
    print(f"     å»¶é²: {best['latency_ms']:.2f}ms, QPS: {best['qps']:.1f}")
    
    for recall_level in [80, 85, 90, 95]:
        key = f"recall_{recall_level}"
        if key in metrics:
            data = metrics[key]
            print(f"  ğŸ“Œ Recallâ‰¥{recall_level}%: Lâ‰¥{data['min_L_search']}, "
                  f"å¯¦éš›={data['actual_recall']:.4f}, QPS={data['qps']:.1f}")
    
    print(f"\nğŸ”¬ èˆ‡ DiskANN è«–æ–‡å°æ¯”:")
    comparison = report["diskann_paper_comparison"]
    print(f"  Recallâ‰¥90%: è«–æ–‡~L=1000, æ‚¨çš„={comparison['your_recall_90']}")
    print(f"  Recallâ‰¥95%: è«–æ–‡~L=2000, æ‚¨çš„={comparison['your_recall_95']}")
    
    print(f"\nâ±ï¸  å»ºç«‹æ€§èƒ½:")
    build = report["build_performance"]
    print(f"  ç¸½å»ºç«‹æ™‚é–“: {build['total_build_time']:.1f}s")
    print(f"  PQ è¨“ç·´: {build['pq_train_time']:.1f}s")
    print(f"  åœ–å»ºç«‹: {build['graph_build_time']:.1f}s")
    
    print("="*80)

def main():
    parser = argparse.ArgumentParser(
        description="DiskRAG åœ¨ GIST-1M æ•¸æ“šé›†ä¸Šçš„å­¸è¡“é©—è­‰",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument('--data_dir', default='data/gist', 
                       help='GIST æ•¸æ“šé›†ç›®éŒ„')
    parser.add_argument('--scale', choices=list(ACADEMIC_SCALES.keys()),
                       default='small_scale', help='è©•ä¼°è¦æ¨¡')
    parser.add_argument('--config', choices=list(DISKANN_CONFIGS.keys()),
                       default='balanced', help='DiskANN é…ç½®')
    parser.add_argument('--output', default='gist_academic_report.json',
                       help='å­¸è¡“å ±å‘Šè¼¸å‡ºæ–‡ä»¶')
    parser.add_argument('--k', type=int, default=10,
                       help='Top-K è©•ä¼°')
    
    args = parser.parse_args()
    
    try:
        # é©—è­‰æ•¸æ“š
        if not verify_gist_data(args.data_dir):
            logger.error("âŒ GIST æ•¸æ“šé›†é©—è­‰å¤±æ•—")
            return 1
        
        # è¼‰å…¥é…ç½®
        scale_config = ACADEMIC_SCALES[args.scale]
        diskann_config = DISKANN_CONFIGS[args.config]
        
        logger.info(f"ğŸš€ é–‹å§‹å­¸è¡“é©—è­‰:")
        logger.info(f"  è¦æ¨¡: {scale_config['description']}")
        logger.info(f"  é…ç½®: {diskann_config['description']}")
        
        # è¼‰å…¥ GIST æ•¸æ“š
        data_dir = Path(args.data_dir)
        logger.info("ğŸ“š è¼‰å…¥ GIST æ•¸æ“š...")
        
        base_full = read_fvecs(data_dir / "gist_base.fvecs")
        learn_full = read_fvecs(data_dir / "gist_learn.fvecs")
        query_full = read_fvecs(data_dir / "gist_query.fvecs")
        gt_full = read_ivecs(data_dir / "gist_groundtruth.ivecs")
        
        # æŒ‰è¦æ¨¡æˆªå–
        base_vectors = base_full[:scale_config["base_size"]]
        learn_vectors = learn_full[:scale_config["learn_size"]]
        query_vectors = query_full[:scale_config["query_size"]]
        
        # è™•ç† Ground Truth
        if scale_config["base_size"] < len(base_full):
            logger.info("ğŸ§¬ é‡æ–°è¨ˆç®—å­é›† Ground Truth...")
            ground_truth = compute_ground_truth(base_vectors, query_vectors, k=100)
        else:
            ground_truth = gt_full[:scale_config["query_size"]]
        
        dataset_info = {
            "base_vectors": len(base_vectors),
            "learn_vectors": len(learn_vectors),
            "query_vectors": len(query_vectors),
            "dimension": base_vectors.shape[1],
            "ground_truth_k": ground_truth.shape[1]
        }
        
        logger.info(f"âœ… æ•¸æ“šè¼‰å…¥å®Œæˆ: {dataset_info}")
        
        # å»ºç«‹ç´¢å¼•
        collection_name = f"gist_academic_{args.scale}_{args.config}"
        build_stats = build_academic_index(
            collection_name, base_vectors, learn_vectors, diskann_config
        )
        
        # è©•ä¼°æ€§èƒ½
        eval_results = evaluate_academic_performance(
            collection_name, query_vectors, ground_truth, k=args.k
        )
        
        # ç”Ÿæˆå ±å‘Š
        report = generate_academic_report(
            scale_config, diskann_config, build_stats, eval_results, dataset_info
        )
        
        # ä¿å­˜å ±å‘Š
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… å­¸è¡“å ±å‘Šå·²ä¿å­˜: {args.output}")
        
        # æ‰“å°æ‘˜è¦
        print_academic_summary(report)
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ å­¸è¡“é©—è­‰å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())