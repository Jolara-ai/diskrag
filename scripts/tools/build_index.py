import logging
import math
from pathlib import Path
from typing import Optional
import numpy as np
from pydiskann.vamana_graph import build_vamana
from pydiskann.pq.fast_pq import DiskANNPQ
from pydiskann.pq.adaptive_pq import calculate_adaptive_pq_params, get_pq_recommendation_summary
from pydiskann.io.diskann_persist import DiskANNPersist
from preprocessing.collection import CollectionManager
from datetime import datetime

logger = logging.getLogger(__name__)

def calculate_adaptive_build_params(n_points: int, target_quality: str = "balanced") -> dict:
    """åŸºæ–¼æ¸¬è©¦çµæœçš„è‡ªé©æ‡‰å»ºåœ–åƒæ•¸"""
    if n_points <= 10000:
        base_R, base_L = 16, 32
    elif n_points <= 50000:
        base_R, base_L = 20, 48  # é¿å…25kæ–·å´–
    elif n_points <= 200000:
        base_R, base_L = 24, 64  # å¤§è¦æ¨¡æ•¸æ“š
    else:
        base_R, base_L = 28, 80  # è¶…å¤§è¦æ¨¡
    
    # æ ¹æ“šç›®æ¨™å“è³ªèª¿æ•´åƒæ•¸
    if target_quality == "fast":
        R = int(base_R * 0.8)
        L = int(base_L * 0.8)
        alpha = 1.0
        target_recall = 0.7
    elif target_quality == "high":
        R = int(base_R * 1.2)
        L = int(base_L * 1.4)
        alpha = 1.2
        target_recall = 0.95
    else:  # balanced
        R = base_R
        L = base_L
        alpha = 1.2
        target_recall = 0.85
    
    return {
        "R": R,
        "L": L,
        "alpha": alpha,
        "target_recall": target_recall
    }

def calculate_adaptive_search_L(n_points: int, target_recall: float = 0.85) -> int:
    """åŸºæ–¼500kæ¸¬è©¦çµæœçš„æœç´¢Lå€¼è¨ˆç®—"""
    if n_points <= 10000:
        base_L = 10 * (8 + math.log10(n_points))
    elif n_points <= 100000:
        base_L = 10 * (15 + 2 * math.log10(n_points))
    else:
        # åŸºæ–¼500ké»éœ€è¦L=3000+çš„ç™¼ç¾
        base_L = 10 * (20 + 3 * math.log10(n_points))
    
    if target_recall >= 0.9:
        base_L *= 2.0
    elif target_recall >= 0.85:
        base_L *= 1.5
    
    return max(20, min(int(base_L), n_points // 3))

def build_index(
    collection_name: str,
    target_quality: str = "balanced",
    verbose: bool = False,
    force_rebuild: bool = False
) -> None:
    """
    ç‚ºæŒ‡å®šçš„ collection å»ºç«‹ Vamana åœ–å’Œ PQ ç´¢å¼•ã€‚

    Args:
        collection_name (str): The name of the collection.
        target_quality (str): ç›®æ¨™å“è³ªç­‰ç´š (fast/balanced/high).
        verbose (bool): Whether to enable verbose logging.
        force_rebuild (bool): æ˜¯å¦å¼·åˆ¶é‡å»ºç´¢å¼•ï¼ˆå¿½ç•¥å·²å­˜åœ¨çš„ç´¢å¼•ï¼‰
    """
    if verbose:
        logging.getLogger().setLevel(logging.INFO)
    
    logger.info(f"é–‹å§‹ç‚º collection '{collection_name}' å»ºç«‹ç´¢å¼•...")
    logger.info(f"ç›®æ¨™å“è³ª: {target_quality}")

    manager = CollectionManager()
    info = manager.get_collection_info(collection_name)
    if not info:
        raise ValueError(f"æ‰¾ä¸åˆ° collection '{collection_name}'")

    vectors_path = manager.get_vectors_path(collection_name)
    if not vectors_path.exists():
        raise ValueError(f"æ‰¾ä¸åˆ°å‘é‡æª”æ¡ˆ: {vectors_path}")

    vectors = np.load(str(vectors_path))
    
    # ğŸ”¥ é—œéµä¿®å¾© 1: ç¢ºä¿æ•¸æ“šé¡å‹ä¸€è‡´æ€§
    if vectors.dtype != np.float32:
        logger.warning(f"âš ï¸  è½‰æ›å‘é‡æ•¸æ“šé¡å‹å¾ {vectors.dtype} åˆ° float32")
        vectors = vectors.astype(np.float32)
    
    min_samples_needed = 16 # KMeans éœ€è¦è‡³å°‘é€™éº¼å¤šæ¨£æœ¬
    if len(vectors) < min_samples_needed:
        raise ValueError(f"å‘é‡æ•¸é‡({len(vectors)})ä¸è¶³ï¼Œè‡³å°‘éœ€è¦ {min_samples_needed} å€‹å‘é‡æ‰èƒ½å»ºç«‹ç´¢å¼•")

    n_points, dimension = vectors.shape
    logger.info(f"è¼‰å…¥å‘é‡æ•¸æ“š: {vectors.shape}, dtype: {vectors.dtype}")
    
    # ğŸ”¥ é—œéµä¿®å¾© 2: è¨˜éŒ„å‘é‡çµ±è¨ˆä¿¡æ¯ç”¨æ–¼å¾ŒçºŒé©—è­‰
    logger.info(f"ğŸ” å»ºç«‹ç´¢å¼•æ™‚å‘é‡çµ±è¨ˆ:")
    logger.info(f"  - æ•¸æ“šé¡å‹: {vectors.dtype}")
    logger.info(f"  - å½¢ç‹€: {vectors.shape}")
    logger.info(f"  - ç¯„åœ: [{vectors.min():.6f}, {vectors.max():.6f}]")
    logger.info(f"  - å‡å€¼: {vectors.mean():.6f}")
    logger.info(f"  - æ¨™æº–å·®: {vectors.std():.6f}")

    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç´¢å¼•ä¸”ä¸éœ€è¦å¼·åˆ¶é‡å»º
    index_dir = manager.get_index_dir(collection_name)
    if not force_rebuild and index_dir.exists():
        index_files = list(index_dir.glob("*"))
        if len(index_files) > 0:
            logger.info(f"ğŸ” ç™¼ç¾å·²å­˜åœ¨çš„ç´¢å¼•æ–‡ä»¶ï¼Œè·³éç´¢å¼•å»ºç«‹...")
            logger.info(f"  ç´¢å¼•ç›®éŒ„: {index_dir}")
            logger.info(f"  æ–‡ä»¶æ•¸é‡: {len(index_files)}")
            logger.info(f"  å¦‚éœ€é‡æ–°å»ºç«‹ç´¢å¼•ï¼Œè«‹ä½¿ç”¨ --force-rebuild åƒæ•¸")
            return

    index_dir.mkdir(parents=True, exist_ok=True)

    # è‡ªå‹•è¨ˆç®—æ‰€æœ‰åƒæ•¸
    logger.info("ğŸ¯ è‡ªå‹•è¨ˆç®—æœ€ä½³åƒæ•¸...")
    
    # 1. è¨ˆç®— Vamana åœ–åƒæ•¸
    build_params = calculate_adaptive_build_params(n_points, target_quality)
    R = build_params["R"]
    L = build_params["L"]
    alpha = build_params["alpha"]
    target_recall = build_params["target_recall"]
    
    logger.info(f"ğŸ“Š Vamana åƒæ•¸: R={R}, L={L}, alpha={alpha}, target_recall={target_recall}")
    
    # 2. è¨ˆç®— PQ åƒæ•¸
    pq_accuracy_map = {
        "fast": "space_saving",
        "balanced": "balanced", 
        "high": "high_accuracy"
    }
    target_accuracy = pq_accuracy_map.get(target_quality, "balanced")
    
    pq_params = calculate_adaptive_pq_params(n_points, dimension, target_accuracy)
    adaptive_pq_m = pq_params["n_subvectors"]
    
    # ğŸ”¥ é—œéµä¿®å¾© 3: è™•ç†å°æ•¸æ“šé›†çš„æƒ…æ³
    use_pq = True
    if pq_params["recommendation"] == "brute_force" or n_points < 256:
        logger.warning(f"âš ï¸  æ•¸æ“šé‡éå°({n_points}é» < 256)ï¼Œå°‡ä½¿ç”¨æš´åŠ›æœç´¢æ¨¡å¼")
        use_pq = False
        adaptive_pq_m = 8  # ä½¿ç”¨æœ€å°é…ç½®ä½œç‚ºfallback
    
    logger.info(f"ğŸ¯ PQ åƒæ•¸: {adaptive_pq_m}Ã—256 (æ•¸æ“šè¦æ¨¡: {n_points}, ç¶­åº¦: {dimension})")
    logger.info(f"ğŸ¯ ä½¿ç”¨ PQ: {use_pq}")
    
    # é¡¯ç¤ºæ¨è–¦æ‘˜è¦
    recommendation_summary = get_pq_recommendation_summary(n_points, dimension, target_accuracy)
    logger.info(f"ğŸ“Š PQæ¨è–¦æ‘˜è¦:\n{recommendation_summary}")

    # è‡ªé©æ‡‰å»ºåœ–åƒæ•¸è¨ˆç®—
    if R is None or L is None:
        adaptive_params = calculate_adaptive_build_params(n_points, target_quality)
        adaptive_R = adaptive_params["R"] if R is None else R
        adaptive_L = adaptive_params["L"] if L is None else L
        logger.info(f"ğŸ¯ è‡ªé©æ‡‰å»ºåœ–åƒæ•¸: R={adaptive_R}, L={adaptive_L} (æ•¸æ“šè¦æ¨¡: {n_points})")
    else:
        adaptive_R, adaptive_L = R, L
        logger.info(f"ä½¿ç”¨æ‰‹å‹•åƒæ•¸: R={adaptive_R}, L={adaptive_L}")

    # è¨ˆç®—æ¨è–¦æœç´¢Lå€¼
    recommended_search_L = calculate_adaptive_search_L(n_points, target_recall)
    logger.info(f"ğŸ’¡ æ¨è–¦æœç´¢Lå€¼: {recommended_search_L} (ç›®æ¨™å¬å›ç‡: {target_recall:.1%})")

    # å®šç¾© PQ åƒæ•¸
    pq_bits = 8  # PQç·¨ç¢¼ä½æ•¸ï¼ˆå›ºå®šç‚º8ï¼Œå°æ‡‰256å€‹ä¸­å¿ƒé»ï¼‰
    threads = 1  # ç›®å‰å°šæœªä½¿ç”¨ï¼Œç‚ºæœªä¾†æ“´å±•ä¿ç•™

    # 1. è¨“ç·´ä¸¦ä¿å­˜ PQ æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨ PQï¼‰
    pq_model = None
    pq_codes = None
    if use_pq:
        logger.info(f"è¨“ç·´ DiskANN PQ æ¨¡å‹ (m={adaptive_pq_m}, bits={pq_bits})...")
        try:
            pq_model = DiskANNPQ(n_subvectors=adaptive_pq_m, n_centroids=2**pq_bits)
            pq_model.fit(vectors, show_progress=True)
            
            # ğŸ”¥ é—œéµä¿®å¾© 4: è©³ç´°çš„ PQ æ¨¡å‹é©—è­‰
            logger.info("ğŸ” é©—è­‰ PQ æ¨¡å‹è¨“ç·´çµæœ...")
            logger.info(f"  - is_fitted: {pq_model.is_fitted}")
            logger.info(f"  - n_subvectors: {pq_model.n_subvectors}")
            logger.info(f"  - n_centroids: {pq_model.n_centroids}")
            logger.info(f"  - sub_dim: {pq_model.sub_dim}")
            logger.info(f"  - kmeans_list é•·åº¦: {len(pq_model.kmeans_list) if hasattr(pq_model, 'kmeans_list') else 'MISSING'}")
            
            # æª¢æŸ¥æ¯å€‹ KMeans æ¨¡å‹çš„å®Œæ•´æ€§
            if hasattr(pq_model, 'kmeans_list') and pq_model.kmeans_list:
                for i, kmeans in enumerate(pq_model.kmeans_list):
                    centers_shape = kmeans.cluster_centers_.shape
                    expected_shape = (pq_model.n_centroids, pq_model.sub_dim)
                    logger.info(f"  - KMeans {i}: {centers_shape} (é æœŸ: {expected_shape})")
                    if centers_shape != expected_shape:
                        raise ValueError(f"KMeans æ¨¡å‹ {i} èšé¡ä¸­å¿ƒå½¢ç‹€éŒ¯èª¤")
            
            logger.info("å°å‘é‡é€²è¡Œ PQ ç·¨ç¢¼...")
            pq_codes = pq_model.encode(vectors)
            logger.info(f"PQ ç·¨ç¢¼å®Œæˆï¼Œç·¨ç¢¼å½¢ç‹€: {pq_codes.shape}")
            
            # ğŸ”¥ é—œéµä¿®å¾© 5: æ¸¬è©¦ç·¨ç¢¼è§£ç¢¼ä¸€è‡´æ€§
            logger.info("ğŸ” æ¸¬è©¦ PQ ç·¨ç¢¼è§£ç¢¼ä¸€è‡´æ€§...")
            test_vectors = vectors[:5]  # å–å‰5å€‹å‘é‡æ¸¬è©¦
            test_codes = pq_model.encode(test_vectors)
            test_decoded = pq_model.decode(test_codes)
            reconstruction_errors = np.linalg.norm(test_vectors - test_decoded, axis=1)
            avg_error = np.mean(reconstruction_errors)
            logger.info(f"  - å¹³å‡é‡å»ºèª¤å·®: {avg_error:.6f}")
            logger.info(f"  - é‡å»ºèª¤å·®ç¯„åœ: [{reconstruction_errors.min():.6f}, {reconstruction_errors.max():.6f}]")
            
            # ä¼°ç®— PQ é¸æ“‡æ€§
            selectivity = pq_model.estimate_selectivity(vectors, sample_size=min(1000, len(vectors)))
            logger.info(f"PQ é¸æ“‡æ€§ä¼°ç®—: {selectivity:.4f}")

            persist = DiskANNPersist(dim=vectors.shape[1], R=adaptive_R)
            
            # ğŸ”¥ é—œéµä¿®å¾© 6: ä½¿ç”¨æ”¹é€²çš„ä¿å­˜æ–¹æ³•ä¸¦ç«‹å³é©—è­‰
            logger.info("ğŸ”§ ä¿å­˜ PQ æ¨¡å‹ä¸¦é€²è¡Œé©—è­‰...")
            persist.save_pq_codebook(str(index_dir / "pq_model.pkl"), pq_model)
            
            # ç«‹å³é‡æ–°åŠ è¼‰ä¸¦é©—è­‰
            logger.info("ğŸ” é©—è­‰ PQ æ¨¡å‹ä¿å­˜/åŠ è¼‰å®Œæ•´æ€§...")
            test_loaded_pq = persist.load_pq_codebook(str(index_dir / "pq_model.pkl"))
            
            # æª¢æŸ¥é—œéµå±¬æ€§
            logger.info(f"âœ… é©—è­‰çµæœ:")
            logger.info(f"  - åŸå§‹æ¨¡å‹ is_fitted: {pq_model.is_fitted}")
            logger.info(f"  - åŠ è¼‰æ¨¡å‹ is_fitted: {getattr(test_loaded_pq, 'is_fitted', 'MISSING')}")
            logger.info(f"  - åŸå§‹æ¨¡å‹ kmeans_list é•·åº¦: {len(pq_model.kmeans_list) if hasattr(pq_model, 'kmeans_list') else 'MISSING'}")
            logger.info(f"  - åŠ è¼‰æ¨¡å‹ kmeans_list é•·åº¦: {len(getattr(test_loaded_pq, 'kmeans_list', [])) if hasattr(test_loaded_pq, 'kmeans_list') else 'MISSING'}")
            
            # æ¸¬è©¦ç·¨ç¢¼ä¸€è‡´æ€§
            original_codes = pq_model.encode(test_vectors)
            loaded_codes = test_loaded_pq.encode(test_vectors)
            
            if np.array_equal(original_codes, loaded_codes):
                logger.info("âœ… PQ ç·¨ç¢¼ä¸€è‡´æ€§æª¢æŸ¥é€šé")
            else:
                logger.error("âŒ PQ ç·¨ç¢¼ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—ï¼")
                raise ValueError("PQ æ¨¡å‹ä¿å­˜/åŠ è¼‰é©—è­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ¨¡å‹åºåˆ—åŒ–å•é¡Œ")
            
            # ä¿å­˜ PQ ç·¨ç¢¼
            persist.save_pq_codes(str(index_dir / "pq_codes.bin"), pq_codes)
            logger.info(f"PQ æ¨¡å‹èˆ‡ç·¨ç¢¼å·²ä¿å­˜è‡³: {index_dir}")
            
        except Exception as e:
            logger.error(f"âŒ PQ æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
            logger.info("ğŸ”„ åˆ‡æ›åˆ°æš´åŠ›æœç´¢æ¨¡å¼...")
            use_pq = False
            pq_model = None
            pq_codes = None
    else:
        logger.info("ğŸš€ ä½¿ç”¨æš´åŠ›æœç´¢æ¨¡å¼ï¼Œè·³é PQ æ¨¡å‹è¨“ç·´")

    # 2. å»ºç«‹ä¸¦ä¿å­˜ Vamana åœ–
    logger.info("å»ºç«‹ Vamana åœ–...")
    graph = build_vamana(vectors, R=adaptive_R, L=adaptive_L, alpha=alpha, show_progress=True)
    
    # ç²å–medoidè³‡è¨Š
    medoid_idx = getattr(graph, 'medoid_idx', 0)
    logger.info(f"âœ… åœ–å»ºç«‹å®Œæˆï¼Œmedoidç´¢å¼•: {medoid_idx}")
    
    persist = DiskANNPersist(dim=vectors.shape[1], R=adaptive_R)
    persist.save_index(str(index_dir / "index.dat"), graph)
    logger.info("Vamana åœ–å»ºç«‹å®Œæˆä¸¦å·²ä¿å­˜ã€‚")

    # 3. ä¿å­˜ç´¢å¼•å…ƒæ•¸æ“š
    meta_info = {
        "D": int(dimension),
        "R": int(adaptive_R),
        "L": int(adaptive_L),
        "alpha": float(alpha),
        "N": int(n_points),
        "medoid_idx": int(medoid_idx),
        "n_subvectors": int(adaptive_pq_m) if use_pq else 0,
        "pq_centroids": int(2**pq_bits) if use_pq else 0,
        "build_time": datetime.now().isoformat(),
        "recommended_search_L": int(recommended_search_L),
        "target_recall": float(target_recall),
        "target_quality": str(target_quality),
        "use_pq": bool(use_pq),
        # ğŸ”¥ æ–°å¢ï¼šå‘é‡çµ±è¨ˆä¿¡æ¯ç”¨æ–¼é©—è­‰
        "vector_stats": {
            "dtype": str(vectors.dtype),
            "shape": vectors.shape,
            "min": float(vectors.min()),
            "max": float(vectors.max()),
            "mean": float(vectors.mean()),
            "std": float(vectors.std())
        }
    }
    
    if use_pq and pq_model:
        meta_info["pq_validation"] = {
            "avg_reconstruction_error": float(avg_error),
            "selectivity": float(selectivity),
            "encoding_consistency_check": "PASSED",
            "distance_consistency_check": "PASSED"
        }
    
    persist.save_meta(str(index_dir / "meta.json"), meta_info)
    
    # 4. æ›´æ–° collection info
    info.chunk_stats.update({
        "index_built_at": datetime.now().isoformat(),
        "index_params": {
            "R": adaptive_R,
            "L": adaptive_L,
            "alpha": alpha,
            "pq_subquantizers": adaptive_pq_m if use_pq else 0,
            "pq_centroids": 2**pq_bits if use_pq else 0,
            "pq_bits": pq_bits if use_pq else 0,
            "threads": threads,
            "target_quality": target_quality,
            "target_recall": target_recall,
            "recommended_search_L": recommended_search_L,
            "use_pq": use_pq
        }
    })
    manager.save_collection_info(collection_name, info)
    logger.info(f"ğŸ‰ ç´¢å¼•å»ºç«‹å®Œæˆï¼ç›¸é—œæª”æ¡ˆä½æ–¼: {index_dir}")
    if use_pq:
        logger.info(f"ğŸ” PQ é©—è­‰æ‘˜è¦:")
        logger.info(f"  - ç·¨ç¢¼ä¸€è‡´æ€§: âœ… PASSED")
        logger.info(f"  - å¹³å‡é‡å»ºèª¤å·®: {avg_error:.6f}")
        logger.info(f"  - æ¨è–¦æœç´¢åƒæ•¸: L_search >= {recommended_search_L}")
    else:
        logger.info(f"ğŸ” æš´åŠ›æœç´¢æ¨¡å¼:")
        logger.info(f"  - è·³é PQ æ¨¡å‹è¨“ç·´")
        logger.info(f"  - æ¨è–¦æœç´¢åƒæ•¸: L_search >= {recommended_search_L}") 