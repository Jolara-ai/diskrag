import logging
from pathlib import Path
import numpy as np
import time
import heapq
import threading
from typing import List, Dict, Any, Optional, Tuple
from pydiskann.io.diskann_persist import MMapNodeReader, DiskANNPersist
from preprocessing.collection import CollectionManager
from preprocessing.config import CollectionInfo, validate_vector_dimension
import json

logger = logging.getLogger(__name__)

class SearchEngineCorrect:
    """ä¿®æ­£å¾Œçš„æœå°‹å¼•æ“ï¼Œå¯¦ç¾æ­£ç¢ºçš„PQåŠ é€Ÿ"""
    
    def __init__(self, collection_name: str, use_thread_safe_stats: bool = True):
        self.collection_name = collection_name
        self.manager = CollectionManager()
        self.info = self.manager.get_collection_info(collection_name)
        if not self.info:
            raise ValueError(f"æ‰¾ä¸åˆ°é›†åˆ: {collection_name}")
        
        index_dir = self.manager.get_index_dir(collection_name)
        index_path = index_dir / "index.dat"
        meta_path = index_dir / "meta.json"
        
        if not index_path.exists() or not meta_path.exists():
            raise ValueError(f"é›†åˆ {collection_name} çš„ç´¢å¼•æª”æ¡ˆä¸å®Œæ•´")
        
        persist = DiskANNPersist(dim=self.info.dimension, R=32)
        self.meta = persist.load_meta(str(meta_path))
        
        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ PQ
        self.use_pq = self.meta.get("use_pq", True)
        
        # åˆå§‹åŒ– PQ ç›¸å…³å±æ€§
        self.pq_model = None
        self.pq_codes = None
        self.n_subvectors = 0
        self.sub_dim = 0
        self.num_centroids = 0
        
        if self.use_pq:
            pq_path = index_dir / "pq_model.pkl"
            pq_codes_path = index_dir / "pq_codes.bin"
            
            if not all(p.exists() for p in [pq_path, pq_codes_path]):
                logger.warning(f"âš ï¸  PQ æ–‡ä»¶ä¸å®Œæ•´ï¼Œåˆ‡æ›åˆ°æš´åŠ›æœç´¢æ¨¡å¼")
                self.use_pq = False
            else:
                try:
                    self.pq_model = persist.load_pq_codebook(str(pq_path))
                    self.pq_codes = persist.load_pq_codes(
                        str(pq_codes_path), 
                        self.meta["N"], 
                        self.meta["n_subvectors"]
                    )
                    self.n_subvectors = self.pq_model.n_subvectors
                    self.sub_dim = self.info.dimension // self.n_subvectors
                    # å…¼å®¹æ–°æ—§ç‰ˆæœ¬çš„PQæ¨¡å‹
                    if hasattr(self.pq_model, 'kmeans_list') and self.pq_model.kmeans_list:
                        self.num_centroids = self.pq_model.kmeans_list[0].n_clusters
                    elif hasattr(self.pq_model, 'n_centroids'):
                        self.num_centroids = self.pq_model.n_centroids
                    else:
                        # é»˜è®¤å€¼
                        self.num_centroids = 256
                except Exception as e:
                    logger.warning(f"âš ï¸  PQ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}ï¼Œåˆ‡æ›åˆ°æš´åŠ›æœç´¢æ¨¡å¼")
                    self.use_pq = False
        
        self.reader = MMapNodeReader(
            str(index_path),
            dim=self.info.dimension,
            R=self.meta.get("R", 32)
        )
        self.medoid_idx = self.meta.get("medoid_idx", 0)
        
        if not validate_vector_dimension(self.info.dimension):
            raise ValueError(
                f"ä¸æ”¯æ´çš„å‘é‡ç¶­åº¦: {self.info.dimension}ã€‚"
                f"è«‹ä½¿ç”¨æ”¯æ´çš„ç¶­åº¦é‡æ–°å»ºç«‹ç´¢å¼•"
            )
        
        # çµ±è¨ˆä¿¡æ¯ - æ·»åŠ ç·šç¨‹é–ä¿è­·
        self.search_stats = {
            'total_searches': 0,
            'total_exact_computations': 0,
            'total_pq_computations': 0,
            'total_search_time': 0.0
        }
        
        # å¯é¸çš„ç·šç¨‹å®‰å…¨çµ±è¨ˆ
        self.use_thread_safe_stats = use_thread_safe_stats
        if use_thread_safe_stats:
            self._stats_lock = threading.Lock()  # æ–°å¢ç·šç¨‹é–
        else:
            self._stats_lock = None
        
        logger.info(
            f"å·²è¼‰å…¥é›†åˆ {collection_name} çš„ç´¢å¼• "
            f"(N={self.meta['N']}, dim={self.info.dimension}, "
            f"ä½¿ç”¨ PQ: {self.use_pq}, "
            f"ç·šç¨‹å®‰å…¨çµ±è¨ˆ: {use_thread_safe_stats})"
        )
        
        if self.use_pq:
            logger.info(f"  PQ é…ç½®: {self.n_subvectors}x{self.num_centroids}")
            # --- â­ï¸ æ–°å¢è¨ºæ–·æ­¥é©Ÿ â­ï¸ ---
            diagnostic_result = self._run_diagnostic_check()
            if not diagnostic_result:
                logger.warning("âš ï¸  PQ è¨ºæ–·æª¢æŸ¥å¤±æ•—ï¼Œä½†ç¹¼çºŒåˆå§‹åŒ–ã€‚å»ºè­°æª¢æŸ¥ PQ æ¨¡å‹ã€‚")
        else:
            logger.info("  ä½¿ç”¨æš´åŠ›æœç´¢æ¨¡å¼")
    
    def _update_stats(self, key: str, value: float = 1):
        """ç·šç¨‹å®‰å…¨çš„çµ±è¨ˆæ›´æ–°æ–¹æ³•"""
        if self.use_thread_safe_stats and self._stats_lock:
            with self._stats_lock:
                self.search_stats[key] += value
        else:
            self.search_stats[key] += value
    
    def _get_stats(self, key: str) -> int:
        """ç·šç¨‹å®‰å…¨çš„çµ±è¨ˆè®€å–æ–¹æ³•"""
        if self.use_thread_safe_stats and self._stats_lock:
            with self._stats_lock:
                return self.search_stats[key]
        else:
            return self.search_stats[key]
    
    def _get_all_stats(self) -> Dict[str, Any]:
        """ç·šç¨‹å®‰å…¨çš„çµ±è¨ˆè®€å–æ–¹æ³•ï¼ˆè¿”å›æ‰€æœ‰çµ±è¨ˆï¼‰"""
        if self.use_thread_safe_stats and self._stats_lock:
            with self._stats_lock:
                return self.search_stats.copy()
        else:
            return self.search_stats.copy()
    
    def _run_diagnostic_check(self):
        """é‹è¡Œæ›´è©³ç´°çš„è¨ºæ–·æª¢æŸ¥"""
        logger.info("ğŸ•µï¸  é‹è¡Œè¨ºæ–·è‡ªæª¢...")
        try:
            # 1. åŸºæœ¬çµ±è¨ˆæª¢æŸ¥
            num_check = min(10, self.meta['N'])
            check_indices = np.random.choice(self.meta['N'], num_check, replace=False)
            original_vectors = np.array([self.reader.get_node(i)[0] for i in check_indices])
            
            logger.info(f"ğŸ“Š åŸå§‹å‘é‡çµ±è¨ˆ:")
            logger.info(f"  - æ•¸æ“šé¡å‹: {original_vectors.dtype}")
            logger.info(f"  - å½¢ç‹€: {original_vectors.shape}")
            logger.info(f"  - ç¯„åœ: [{original_vectors.min():.4f}, {original_vectors.max():.4f}]")
            logger.info(f"  - å‡å€¼: {original_vectors.mean():.4f}")
            logger.info(f"  - æ¨™æº–å·®: {original_vectors.std():.4f}")
            
            # 2. æ ¹æ“šæœç´¢æ¨¡å¼é€²è¡Œä¸åŒçš„æª¢æŸ¥
            if self.use_pq:
                logger.info("ğŸ” é€²è¡Œ PQ æ¨¡å¼è¨ºæ–·æª¢æŸ¥...")
                # æª¢æŸ¥ PQ æ¨¡å‹å®Œæ•´æ€§
                if not hasattr(self, 'pq_model') or not self.pq_model:
                    logger.error("âŒ PQ æ¨¡å‹ä¸å­˜åœ¨ï¼")
                    return False
                
                if not hasattr(self.pq_model, 'kmeans_list') or not self.pq_model.kmeans_list:
                    logger.error("âŒ PQ æ¨¡å‹ç¼ºå°‘ kmeans_listï¼é€™æ˜¯å°è‡´ recall=0 çš„ä¸»è¦åŸå› ï¼")
                    return False
                
                if len(self.pq_model.kmeans_list) != self.n_subvectors:
                    logger.error(f"âŒ PQ å­å‘é‡æ•¸é‡ä¸åŒ¹é…: é æœŸ {self.n_subvectors}, å¯¦éš› {len(self.pq_model.kmeans_list)}")
                    return False
                
                # 3. æª¢æŸ¥è³ªå¿ƒæ˜¯å¦ç‚ºé›¶
                for i, kmeans in enumerate(self.pq_model.kmeans_list):
                    centroids = kmeans.cluster_centers_
                    if np.allclose(centroids, 0):
                        logger.error(f"âŒ å­å‘é‡ {i} çš„è³ªå¿ƒå…¨ç‚ºé›¶ï¼")
                        return False
                
                logger.info("âœ… PQ æ¨¡å‹çµæ§‹æª¢æŸ¥é€šé")
                
                # 4. è·é›¢è¨ˆç®—ä¸€è‡´æ€§æª¢æŸ¥
                query_vector = original_vectors[0]  # ä½¿ç”¨ç¬¬ä¸€å€‹å‘é‡ä½œç‚ºæŸ¥è©¢
                
                # æ§‹å»º PQ æŸ¥æ‰¾è¡¨
                try:
                    pq_lut = self._build_pq_lut_fixed(query_vector)
                    logger.info(f"âœ… PQ æŸ¥æ‰¾è¡¨æ§‹å»ºæˆåŠŸ: å½¢ç‹€ {pq_lut.shape}")
                except Exception as e:
                    logger.error(f"âŒ PQ æŸ¥æ‰¾è¡¨æ§‹å»ºå¤±æ•—: {e}")
                    return False
                
                # æ¯”è¼ƒå¹¾å€‹é»çš„ç²¾ç¢ºè·é›¢å’Œ PQ è·é›¢
                logger.info("ğŸ” è·é›¢è¨ˆç®—ä¸€è‡´æ€§æª¢æŸ¥:")
                distance_correlations = []
                
                for i in range(min(5, num_check)):
                    node_id = check_indices[i]
                    
                    # ç²¾ç¢ºè·é›¢
                    exact_dist = self._compute_exact_distance(query_vector, node_id)
                    
                    # PQ è·é›¢
                    if hasattr(self, 'pq_codes') and node_id < len(self.pq_codes):
                        pq_code = self.pq_codes[node_id]
                        pq_dist = self._get_pq_distance(pq_lut, pq_code)
                        ratio = pq_dist / exact_dist if exact_dist > 0 else float('inf')
                        distance_correlations.append((exact_dist, pq_dist))
                        
                        logger.info(f"  Node {node_id}: Exact={exact_dist:.6f}, PQ={pq_dist:.6f}, Ratio={ratio:.3f}")
                        
                        # æª¢æŸ¥æ˜¯å¦åˆç†
                        if ratio < 0.1 or ratio > 10:
                            logger.warning(f"âš ï¸  Node {node_id} çš„è·é›¢æ¯”ä¾‹ç•°å¸¸: {ratio:.3f}")
                    else:
                        logger.warning(f"âš ï¸  Node {node_id} æ²’æœ‰å°æ‡‰çš„ PQ ç·¨ç¢¼")
                
                # è¨ˆç®—ç›¸é—œæ€§
                if distance_correlations:
                    exact_dists, pq_dists = zip(*distance_correlations)
                    correlation = np.corrcoef(exact_dists, pq_dists)[0, 1]
                    logger.info(f"ğŸ“Š ç²¾ç¢ºè·é›¢èˆ‡ PQ è·é›¢çš„ç›¸é—œæ€§: {correlation:.4f}")
                    
                    if correlation < 0.5:
                        logger.error(f"âŒ è·é›¢ç›¸é—œæ€§éä½ ({correlation:.4f})ï¼Œé€™æ˜¯å°è‡´ recall=0 çš„åŸå› ï¼")
                        return False
                
                logger.info("âœ… è·é›¢è¨ˆç®—ä¸€è‡´æ€§æª¢æŸ¥é€šé")
            else:
                logger.info("ğŸ” é€²è¡Œæš´åŠ›æœç´¢æ¨¡å¼è¨ºæ–·æª¢æŸ¥...")
                # æš´åŠ›æœç´¢æ¨¡å¼åªéœ€è¦æª¢æŸ¥åŸºæœ¬åŠŸèƒ½
                query_vector = original_vectors[0]
                
                # æ¸¬è©¦ç²¾ç¢ºè·é›¢è¨ˆç®—
                test_distances = []
                for i in range(min(5, num_check)):
                    node_id = check_indices[i]
                    exact_dist = self._compute_exact_distance(query_vector, node_id)
                    test_distances.append(exact_dist)
                    logger.info(f"  Node {node_id}: Distance={exact_dist:.6f}")
                
                if not test_distances:
                    logger.error("âŒ ç„¡æ³•è¨ˆç®—ä»»ä½•è·é›¢ï¼")
                    return False
                
                logger.info("âœ… æš´åŠ›æœç´¢æ¨¡å¼è¨ºæ–·æª¢æŸ¥é€šé")
            
            return True
            
        except Exception as e:
            logger.error(f"è¨ºæ–·éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return False
    
    def __del__(self):
        if hasattr(self, 'reader'):
            self.reader.close()
    
    def _build_pq_lut(self, query_vector: np.ndarray) -> np.ndarray:
        """æ§‹å»ºPQæŸ¥æ‰¾è¡¨ - ä½¿ç”¨DiskANNPQçš„ADCæ–¹æ³•"""
        # æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶è°ƒç”¨ç›¸åº”æ–¹æ³•
        if hasattr(self.pq_model, 'compute_distance_table'):
            return self.pq_model.compute_distance_table(query_vector)
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œæ‰‹åŠ¨æ„å»ºè·ç¦»è¡¨
            lut = np.empty((self.n_subvectors, self.num_centroids), dtype=np.float32)
            for i in range(self.n_subvectors):
                start_idx = i * self.sub_dim
                end_idx = (i + 1) * self.sub_dim
                sub_query = query_vector[start_idx:end_idx]
                # å…¼å®¹æ–°æ—§ç‰ˆæœ¬çš„PQæ¨¡å‹
                if hasattr(self.pq_model, 'kmeans_list') and i < len(self.pq_model.kmeans_list):
                    centroids = self.pq_model.kmeans_list[i].cluster_centers_
                else:
                    # å¦‚æœkmeans_listä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    centroids = np.zeros((self.num_centroids, self.sub_dim))
                diff = centroids - sub_query[np.newaxis, :]
                lut[i, :] = np.sum(diff * diff, axis=1)
            return lut

    def _build_pq_lut_fixed(self, query_vector: np.ndarray) -> np.ndarray:
        """ä¿®å¾©çš„ PQ æŸ¥æ‰¾è¡¨æ§‹å»ºæ–¹æ³•"""
        # é¦–å…ˆæª¢æŸ¥ PQ æ¨¡å‹æ˜¯å¦æœ‰å…§å»ºæ–¹æ³•
        if hasattr(self.pq_model, 'compute_distance_table'):
            try:
                return self.pq_model.compute_distance_table(query_vector)
            except Exception as e:
                logger.warning(f"ä½¿ç”¨å…§å»º distance_table æ–¹æ³•å¤±æ•—: {e}ï¼Œæ”¹ç”¨æ‰‹å‹•æ§‹å»º")
        
        # æ‰‹å‹•æ§‹å»º - ä½†è¦ç¢ºä¿è³ªå¿ƒæ­£ç¢º
        if not hasattr(self.pq_model, 'kmeans_list') or not self.pq_model.kmeans_list:
            raise ValueError("PQ æ¨¡å‹ç¼ºå°‘ kmeans_listï¼Œç„¡æ³•é€²è¡Œè·é›¢è¨ˆç®—")
        
        if len(self.pq_model.kmeans_list) != self.n_subvectors:
            raise ValueError(f"PQ å­å‘é‡æ•¸é‡ä¸åŒ¹é…: é æœŸ {self.n_subvectors}, å¯¦éš› {len(self.pq_model.kmeans_list)}")
        
        lut = np.empty((self.n_subvectors, self.num_centroids), dtype=np.float32)
        
        for i in range(self.n_subvectors):
            start_idx = i * self.sub_dim
            end_idx = (i + 1) * self.sub_dim
            sub_query = query_vector[start_idx:end_idx]
            
            # ç¢ºä¿è³ªå¿ƒå­˜åœ¨ä¸”ä¸ç‚ºé›¶
            kmeans = self.pq_model.kmeans_list[i]
            centroids = kmeans.cluster_centers_
            
            if centroids.shape[0] != self.num_centroids:
                raise ValueError(f"å­å‘é‡ {i} çš„è³ªå¿ƒæ•¸é‡ä¸åŒ¹é…: é æœŸ {self.num_centroids}, å¯¦éš› {centroids.shape[0]}")
            
            if np.allclose(centroids, 0):
                raise ValueError(f"å­å‘é‡ {i} çš„è³ªå¿ƒå…¨ç‚ºé›¶")
            
            # è¨ˆç®—å¹³æ–¹è·é›¢
            diff = centroids - sub_query[np.newaxis, :]
            lut[i, :] = np.sum(diff * diff, axis=1)
        
        return lut

    def _debug_search_step_by_step(self, query_vector: np.ndarray, k: int = 5) -> Dict:
        """é€æ­¥èª¿è©¦æœç´¢éç¨‹"""
        logger.info("ğŸ” é–‹å§‹é€æ­¥èª¿è©¦æœç´¢éç¨‹...")
        
        # 1. æª¢æŸ¥ medoid
        logger.info(f"ğŸ¯ Medoid ç´¢å¼•: {self.medoid_idx}")
        medoid_exact_dist = self._compute_exact_distance(query_vector, self.medoid_idx)
        logger.info(f"ğŸ¯ Medoid ç²¾ç¢ºè·é›¢: {medoid_exact_dist:.6f}")
        
        # 2. æ§‹å»º PQ æŸ¥æ‰¾è¡¨
        try:
            pq_lut = self._build_pq_lut_fixed(query_vector)
            logger.info(f"âœ… PQ æŸ¥æ‰¾è¡¨æ§‹å»ºæˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ PQ æŸ¥æ‰¾è¡¨æ§‹å»ºå¤±æ•—: {e}")
            return {"error": str(e)}
        
        # 3. æª¢æŸ¥ medoid çš„é„°å±…
        _, medoid_neighbors = self.reader.get_node(self.medoid_idx)
        logger.info(f"ğŸ”— Medoid æœ‰ {len([n for n in medoid_neighbors if n >= 0])} å€‹æœ‰æ•ˆé„°å±…")
        
        # 4. æª¢æŸ¥å‰å¹¾å€‹é„°å±…çš„è·é›¢
        valid_neighbors = [n for n in medoid_neighbors if n >= 0 and n < len(self.pq_codes)][:5]
        
        neighbor_info = []
        for neighbor_id in valid_neighbors:
            exact_dist = self._compute_exact_distance(query_vector, neighbor_id)
            pq_code = self.pq_codes[neighbor_id]
            pq_dist = self._get_pq_distance(pq_lut, pq_code)
            
            neighbor_info.append({
                'id': neighbor_id,
                'exact_dist': exact_dist,
                'pq_dist': pq_dist,
                'ratio': pq_dist / exact_dist if exact_dist > 0 else float('inf')
            })
            
            logger.info(f"  é„°å±… {neighbor_id}: Exact={exact_dist:.6f}, PQ={pq_dist:.6f}, Ratio={pq_dist/exact_dist:.3f}")
        
        return {
            "medoid_idx": self.medoid_idx,
            "medoid_exact_dist": medoid_exact_dist,
            "neighbor_info": neighbor_info
        }
    
    def _get_pq_distance(self, lut: np.ndarray, pq_code: np.ndarray) -> float:
        """è¨ˆç®—PQè·é›¢ - ä½¿ç”¨DiskANNPQçš„ADCæ–¹æ³•"""
        # æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶è°ƒç”¨ç›¸åº”æ–¹æ³•
        if hasattr(self.pq_model, 'asymmetric_distance'):
            return self.pq_model.asymmetric_distance(pq_code.reshape(1, -1), lut)[0]
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œæ‰‹åŠ¨è®¡ç®—è·ç¦»
            return np.sum(lut[np.arange(self.n_subvectors), pq_code])
    
    def _compute_exact_distance(self, query_vector: np.ndarray, node_id: int) -> float:
        """è¨ˆç®—ç²¾ç¢ºçš„L2è·é›¢"""
        self._update_stats('total_exact_computations')
        full_vector, _ = self.reader.get_node(node_id)
        diff = full_vector - query_vector
        return np.sum(diff * diff)
    
    def _should_compute_exact_distance(self, pq_distance: float, current_candidates: List, 
                                     L: int, current_best_distance: float) -> bool:
        """æ±ºå®šæ˜¯å¦éœ€è¦è¨ˆç®—ç²¾ç¢ºè·é›¢çš„ç­–ç•¥"""
        
        # ç­–ç•¥1: å¦‚æœå€™é¸åˆ—è¡¨æœªæ»¿ï¼Œç¸½æ˜¯è¨ˆç®—ç²¾ç¢ºè·é›¢
        if len(current_candidates) < L:
            return True
        
        # ç­–ç•¥2: å¦‚æœPQè·é›¢æ˜é¡¯å„ªæ–¼ç•¶å‰æœ€å·®å€™é¸ï¼Œè¨ˆç®—ç²¾ç¢ºè·é›¢
        if pq_distance < current_best_distance * 0.8:  # 80%é–¾å€¼
            return True
        
        # ç­–ç•¥3: å°æ–¼é‚Šç•Œæƒ…æ³ï¼Œä»¥ä¸€å®šæ¦‚ç‡è¨ˆç®—ç²¾ç¢ºè·é›¢ï¼ˆé¿å…èª¤åˆ¤ï¼‰
        if pq_distance < current_best_distance * 1.2:  # 120%é–¾å€¼å…§
            return np.random.random() < 0.2  # 20%æ¦‚ç‡
        
        return False
    def _pq_accelerated_graph_search(self, query_vector: np.ndarray, k: int = 10, 
                                   L: int = 100, beam_width: int = None) -> Tuple[List[Tuple[float, int]], Dict]:
        """
        ä½¿ç”¨PQåŠ é€Ÿçš„åœ–æœç´¢ - æ­£ç¢ºçš„DiskANN-PQå¯¦ç¾
        
        é—œéµï¼šä»ç„¶æ²¿è‘—åœ–çš„é‚Šé€²è¡Œæœç´¢ï¼Œä½†ä½¿ç”¨PQä¾†æ¸›å°‘ç²¾ç¢ºè·é›¢è¨ˆç®—
        """
        start_time = time.time()
        self._update_stats('total_searches')
        
        # é‡ç½®è¨ˆæ•¸å™¨ - éœ€è¦ç·šç¨‹å®‰å…¨åœ°è®€å–
        initial_exact_count = self._get_stats('total_exact_computations')
        initial_pq_count = self._get_stats('total_pq_computations')
        
        # æ§‹å»ºPQæŸ¥æ‰¾è¡¨
        pq_lut = self._build_pq_lut_fixed(query_vector)
        
        # åˆå§‹åŒ–æœç´¢
        visited = {self.medoid_idx}
        
        # å€™é¸éšŠåˆ—ï¼š(distance, node_id)ï¼Œå°é ‚å †
        candidates = []
        # çµæœéšŠåˆ—ï¼š(-distance, node_id)ï¼Œå¤§é ‚å †ç”¨æ–¼ç¶­è­·top-L
        results = []
        
        # å¾medoidé–‹å§‹
        exact_dist = self._compute_exact_distance(query_vector, self.medoid_idx)
        heapq.heappush(candidates, (exact_dist, self.medoid_idx))
        heapq.heappush(results, (-exact_dist, self.medoid_idx))
        
        search_steps = 0
        max_search_steps = min(L * 10, self.meta["N"])  # é¿å…ç„¡é™å¾ªç’°
        
        while candidates and search_steps < max_search_steps:
            search_steps += 1
            
            # å–å‡ºç•¶å‰æœ€ä½³å€™é¸
            current_dist, current_node = heapq.heappop(candidates)
            
            # å‰ªæï¼šå¦‚æœç•¶å‰è·é›¢å·²ç¶“æ¯”çµæœé›†ä¸­æœ€å·®çš„é‚„å·®ï¼Œå¯ä»¥åœæ­¢
            if len(results) >= L and current_dist > -results[0][0]:
                break
            
            # ç²å–ç•¶å‰ç¯€é»çš„é„°å±…
            _, neighbors = self.reader.get_node(current_node)
            
            for neighbor_id in neighbors:
                if neighbor_id < 0 or neighbor_id in visited:
                    continue
                
                visited.add(neighbor_id)
                
                # é—œéµæ­¥é©Ÿï¼šé¦–å…ˆä½¿ç”¨PQä¼°ç®—è·é›¢
                if neighbor_id < len(self.pq_codes):
                    pq_code = self.pq_codes[neighbor_id]
                    pq_distance = self._get_pq_distance(pq_lut, pq_code)
                    self._update_stats('total_pq_computations')
                else:
                    # å¦‚æœæ²’æœ‰PQç·¨ç¢¼ï¼Œç›´æ¥è¨ˆç®—ç²¾ç¢ºè·é›¢
                    pq_distance = float('inf')
                
                # æ±ºå®šæ˜¯å¦è¨ˆç®—ç²¾ç¢ºè·é›¢
                current_best_distance = -results[0][0] if results else float('inf')
                
                if (neighbor_id >= len(self.pq_codes) or 
                    self._should_compute_exact_distance(pq_distance, results, L, current_best_distance)):
                    # è¨ˆç®—ç²¾ç¢ºè·é›¢
                    exact_distance = self._compute_exact_distance(query_vector, neighbor_id)
                    
                    # æ›´æ–°å€™é¸å’ŒçµæœéšŠåˆ—
                    if len(results) < L or exact_distance < -results[0][0]:
                        heapq.heappush(candidates, (exact_distance, neighbor_id))
                        heapq.heappush(results, (-exact_distance, neighbor_id))
                        
                        # ç¶­è­·çµæœéšŠåˆ—å¤§å°
                        if len(results) > L:
                            heapq.heappop(results)
            
            # ç¶­è­·å€™é¸éšŠåˆ—å¤§å°ï¼ˆbeam searchï¼‰
            if beam_width and len(candidates) > beam_width:
                candidates = heapq.nsmallest(beam_width, candidates)
                heapq.heapify(candidates)
        
        # æå–æœ€çµ‚çµæœ
        final_results = []
        for neg_dist, node_id in results:
            final_results.append((-neg_dist, node_id))
        
        # æŒ‰è·é›¢æ’åºä¸¦å–å‰kå€‹
        final_results.sort(key=lambda x: x[0])
        top_k_results = final_results[:k]
        
        search_time = time.time() - start_time
        self._update_stats('total_search_time', search_time)
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯ - éœ€è¦ç·šç¨‹å®‰å…¨åœ°è®€å–
        exact_computations_this_search = self._get_stats('total_exact_computations') - initial_exact_count
        pq_computations_this_search = self._get_stats('total_pq_computations') - initial_pq_count
        
        search_stats = {
            'search_time': search_time,
            'nodes_visited': len(visited),
            'exact_distance_computations': exact_computations_this_search,
            'pq_distance_computations': pq_computations_this_search,
            'computation_reduction_rate': 1 - (exact_computations_this_search / max(1, pq_computations_this_search)),
            'search_steps': search_steps
        }
        
        return top_k_results, search_stats
    
    def _exact_graph_search(self, query_vector: np.ndarray, k: int = 10, L: int = 100) -> Tuple[List[Tuple[float, int]], Dict]:
        """åŸºæº–çš„ç²¾ç¢ºåœ–æœç´¢ï¼ˆæ¯å€‹é„°å±…éƒ½è¨ˆç®—ç²¾ç¢ºè·é›¢ï¼‰"""
        from pydiskann.vamana_graph import beam_search_from_disk
        
        start_time = time.time()
        results = beam_search_from_disk(
            self.reader,
            query_vector,
            start_id=self.medoid_idx,
            beam_width=8,
            k=k
        )
        search_time = time.time() - start_time
        
        search_stats = {
            'search_time': search_time,
            'exact_distance_computations': len(results) * 2,  # ä¼°ç®—
            'search_type': 'exact_beam_search'
        }
        
        return results, search_stats
    
    def search(self, query: str, k: int = 5, beam_width: int = 8,
               embedding_fn: Optional[callable] = None, L_search: int = None,
               use_pq_search: bool = True, use_simple_pq: bool = False) -> Dict[str, Any]:
        """
        æœç´¢æ¥å£ - æ”¯æŒæš´åŠ›æœç´¢æ¨¡å¼
        """
        if embedding_fn is None:
            raise ValueError("å¿…é ˆæä¾› embedding_fn ä¾†ç”¢ç”ŸæŸ¥è©¢å‘é‡")
        
        if L_search is None:
            L_search = max(k * 2, 20)
        
        total_start_time = time.time()
        embedding_start_time = time.time()
        query_vector = embedding_fn(query)
        embedding_time = time.time() - embedding_start_time
        
        if query_vector.shape[0] != self.info.dimension:
            raise ValueError(
                f"æŸ¥è©¢å‘é‡ç¶­åº¦ä¸åŒ¹é…: é æœŸ {self.info.dimension}ï¼Œ"
                f"å¯¦éš› {query_vector.shape[0]}"
            )
        
        try:
            if use_simple_pq:
                # ä¸å†æ”¯æ´éŒ¯èª¤çš„simple PQå¯¦ç¾
                logger.warning("simple_pq é¸é …å·²è¢«ç§»é™¤ï¼Œä½¿ç”¨æ­£ç¢ºçš„PQåŠ é€Ÿåœ–æœç´¢")
                use_pq_search = True
            
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ PQ æœç´¢
            if use_pq_search and not self.use_pq:
                logger.info("ğŸ”„ æª¢æ¸¬åˆ°æš´åŠ›æœç´¢æ¨¡å¼ï¼Œè‡ªå‹•åˆ‡æ›åˆ°ç²¾ç¢ºæœç´¢")
                use_pq_search = False
            
            if use_pq_search and self.use_pq:
                # ä½¿ç”¨æ­£ç¢ºçš„PQåŠ é€Ÿåœ–æœç´¢
                top_k_results, search_stats = self._pq_accelerated_graph_search(
                    query_vector, k, L_search, beam_width
                )
            else:
                # ä½¿ç”¨ç²¾ç¢ºåœ–æœç´¢
                top_k_results, search_stats = self._exact_graph_search(
                    query_vector, k, L_search
                )
            
            # ç²å–æ–‡æœ¬çµæœ
            search_results = []
            for dist, idx in top_k_results:
                text_data = self.manager.get_text_by_index(self.collection_name, idx)
                if text_data:
                    text, metadata = text_data
                    if not isinstance(metadata, dict):
                        metadata = {"id": idx, "text": text}
                    search_results.append({
                        "text": text,
                        "distance": float(dist),
                        "metadata": metadata
                    })
            
            total_time = time.time() - total_start_time
            
            # çµ„ç¹”è¿”å›çµæœ
            timing = {
                'embedding_time': embedding_time,
                'search_time': search_stats.get('search_time', 0),
                'total_time': total_time
            }
            
            # æ·»åŠ æœç´¢çµ±è¨ˆä¿¡æ¯
            stats = {
                'search_type': 'pq_accelerated' if (use_pq_search and self.use_pq) else 'exact',
                'nodes_visited': search_stats.get('nodes_visited', 0),
                'k': k,
                'L_search': L_search
            }
            
            return {
                "results": search_results,
                "timing": timing,
                "stats": stats
            }
            
        except Exception as e:
            logger.error(f"æœç´¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

    def search_with_debug(self, query: str, k: int = 5, beam_width: int = 8,
                         embedding_fn: Optional[callable] = None, L_search: int = None,
                         use_pq_search: bool = True, debug_mode: bool = False) -> Dict[str, Any]:
        """å¸¶èª¿è©¦åŠŸèƒ½çš„æœç´¢æ–¹æ³•"""
        
        if embedding_fn is None:
            raise ValueError("å¿…é ˆæä¾› embedding_fn ä¾†ç”¢ç”ŸæŸ¥è©¢å‘é‡")
        
        if L_search is None:
            L_search = max(k * 2, 20)
        
        query_vector = embedding_fn(query)
        
        if debug_mode:
            # é‹è¡Œè©³ç´°è¨ºæ–·
            diagnostic_result = self._run_diagnostic_check()
            if not diagnostic_result:
                logger.error("âŒ è¨ºæ–·æª¢æŸ¥å¤±æ•—ï¼Œæœç´¢å¯èƒ½ä¸æœƒæ­£å¸¸å·¥ä½œ")
            
            # é€æ­¥èª¿è©¦
            debug_info = self._debug_search_step_by_step(query_vector, k)
            
            # æ¯”è¼ƒç²¾ç¢ºæœç´¢å’Œ PQ æœç´¢
            logger.info("ğŸ”„ æ¯”è¼ƒç²¾ç¢ºæœç´¢å’Œ PQ æœç´¢çµæœ...")
            
            exact_results, _ = self._exact_graph_search(query_vector, k, L_search)
            logger.info(f"ç²¾ç¢ºæœç´¢çµæœ: {[idx for _, idx in exact_results[:k]]}")
            
            if use_pq_search:
                try:
                    pq_results, _ = self._pq_accelerated_graph_search(query_vector, k, L_search, beam_width)
                    logger.info(f"PQ æœç´¢çµæœ: {[idx for _, idx in pq_results[:k]]}")
                except Exception as e:
                    logger.error(f"PQ æœç´¢å¤±æ•—: {e}")
                    pq_results = []
            
            return {
                "debug_info": debug_info,
                "exact_results": exact_results,
                "pq_results": pq_results if use_pq_search else [],
                "diagnostic_passed": diagnostic_result
            }
        
        # æ­£å¸¸æœç´¢æµç¨‹
        return self.search(query, k, beam_width, embedding_fn, L_search, use_pq_search)

    def get_search_statistics(self) -> Dict[str, Any]:
        """ç²å–æœç´¢çµ±è¨ˆä¿¡æ¯"""
        all_stats = self._get_all_stats()
        if all_stats['total_searches'] == 0:
            return {"message": "å°šæœªåŸ·è¡Œä»»ä½•æœç´¢"}
            
        avg_exact_per_search = all_stats['total_exact_computations'] / all_stats['total_searches']
        avg_pq_per_search = all_stats['total_pq_computations'] / all_stats['total_searches']
        avg_search_time = all_stats['total_search_time'] / all_stats['total_searches']
        
        return {
            'total_searches': all_stats['total_searches'],
            'avg_exact_computations_per_search': avg_exact_per_search,
            'avg_pq_computations_per_search': avg_pq_per_search,
            'avg_search_time': avg_search_time,
            'total_exact_computations': all_stats['total_exact_computations'],
            'total_pq_computations': all_stats['total_pq_computations'],
            'overall_computation_reduction_rate': 1 - (avg_exact_per_search / max(1, avg_pq_per_search))
        }
    
    def get_text_by_hash(self, text_hash: str) -> Optional[Tuple[str, Dict[str, Any]]]:
        return self.manager.get_text_by_hash(self.collection_name, text_hash)
    
    @classmethod
    def list_collections(cls) -> List[CollectionInfo]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„é›†åˆ"""
        return CollectionManager().list_collections()
    
    def get_collection_info(self) -> CollectionInfo:
        """å–å¾—ç•¶å‰é›†åˆçš„è³‡è¨Š"""
        return self.info

    def faq_search(self, query: str, k: int = 5, beam_width: int = 8,
                   embedding_fn: Optional[callable] = None, L_search: int = None,
                   use_pq_search: bool = True) -> Dict[str, Any]:
        """
        FAQ å°ˆç”¨æœç´¢ - æ”¯æŒæš´åŠ›æœç´¢æ¨¡å¼
        """
        if embedding_fn is None:
            raise ValueError("å¿…é ˆæä¾› embedding_fn ä¾†ç”¢ç”ŸæŸ¥è©¢å‘é‡")
        
        if L_search is None:
            L_search = max(k * 2, 20)
        
        total_start_time = time.time()
        embedding_start_time = time.time()
        query_vector = embedding_fn(query)
        embedding_time = time.time() - embedding_start_time
        
        if query_vector.shape[0] != self.info.dimension:
            raise ValueError(
                f"æŸ¥è©¢å‘é‡ç¶­åº¦ä¸åŒ¹é…: é æœŸ {self.info.dimension}ï¼Œ"
                f"å¯¦éš› {query_vector.shape[0]}"
            )
        
        try:
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ PQ æœç´¢
            if use_pq_search and not self.use_pq:
                logger.info("ğŸ”„ æª¢æ¸¬åˆ°æš´åŠ›æœç´¢æ¨¡å¼ï¼Œè‡ªå‹•åˆ‡æ›åˆ°ç²¾ç¢ºæœç´¢")
                use_pq_search = False
            
            # ç²å–æ›´å¤šçµæœä»¥ä¾¿å»é‡
            search_k = k * 3
            
            if use_pq_search and self.use_pq:
                top_k_results, search_stats = self._pq_accelerated_graph_search(
                    query_vector, search_k, L_search, beam_width
                )
            else:
                top_k_results, search_stats = self._exact_graph_search(
                    query_vector, search_k, L_search
                )
            
            # è™•ç†çµæœå»é‡
            final_results = []
            seen_qa_ids = set()
            
            for dist, idx in top_k_results:
                text_data = self.manager.get_text_by_index(self.collection_name, idx)
                if not text_data:
                    continue
                
                text, metadata = text_data
                
                # è§£æå…ƒæ•¸æ“š
                if isinstance(metadata, str):
                    try:
                        metadata = json.loads(metadata)
                    except json.JSONDecodeError:
                        metadata = {"id": idx, "text": text}
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºFAQé¡å‹ - æ”¯æŒåµŒå¥—metadataçµæ§‹
                metadata_type = metadata.get("type")
                if not metadata_type:
                    # æª¢æŸ¥åµŒå¥—çš„metadataå­—æ®µ
                    nested_metadata = metadata.get("metadata")
                    if isinstance(nested_metadata, str):
                        try:
                            nested_metadata = json.loads(nested_metadata)
                            metadata_type = nested_metadata.get("type")
                        except json.JSONDecodeError:
                            pass
                    elif isinstance(nested_metadata, dict):
                        metadata_type = nested_metadata.get("type")
                
                if metadata_type != "faq":
                    continue
                
                # ç²å–qa_id
                qa_id = metadata.get("qa_id")
                if not qa_id or qa_id in seen_qa_ids:
                    continue  # è·³éæ²’æœ‰qa_idæˆ–å·²ç¶“è™•ç†éçš„
                
                seen_qa_ids.add(qa_id)
                
                final_results.append({
                    "text": text,
                    "distance": float(dist),
                    "metadata": metadata
                })
                
                if len(final_results) >= k:
                    break
            
            total_time = time.time() - total_start_time
            
            # çµ„ç¹”è¿”å›çµæœ
            timing = {
                'embedding_time': embedding_time,
                'search_time': search_stats.get('search_time', 0),
                'total_time': total_time
            }
            
            stats = {
                'search_type': 'pq_accelerated' if (use_pq_search and self.use_pq) else 'exact',
                'nodes_visited': search_stats.get('nodes_visited', 0),
                'k': k,
                'L_search': L_search,
                'total_results_before_dedup': len(top_k_results),
                'final_results_after_dedup': len(final_results)
            }
            
            return {
                "results": final_results,
                "timing": timing,
                "stats": stats
            }
            
        except Exception as e:
            logger.error(f"FAQæœç´¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise


# å‘å¾Œå…¼å®¹çš„åˆ¥åï¼Œä½†æ‡‰è©²é€æ­¥é·ç§»åˆ°æ–°çš„å¯¦ç¾
SearchEngine = SearchEngineCorrect

def performance_test_search_engine(collection_name: str, num_queries: int = 100):
    """
    æ€§èƒ½æ¸¬è©¦ï¼šæ¯”è¼ƒç·šç¨‹å®‰å…¨ä¿®æ”¹å‰å¾Œçš„æ€§èƒ½å·®ç•°
    
    Args:
        collection_name: é›†åˆåç¨±
        num_queries: æ¸¬è©¦æŸ¥è©¢æ•¸é‡
    """
    import time
    import threading
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    # å‰µå»ºæœç´¢å¼•æ“å¯¦ä¾‹
    engine = SearchEngineCorrect(collection_name)
    
    # æ¨¡æ“¬æŸ¥è©¢å‘é‡ï¼ˆä½¿ç”¨éš¨æ©Ÿå‘é‡ï¼‰
    def mock_embedding(query: str) -> np.ndarray:
        """æ¨¡æ“¬embeddingå‡½æ•¸"""
        return np.random.randn(engine.info.dimension).astype(np.float32)
    
    def single_search(query_id: int) -> float:
        """å–®æ¬¡æœç´¢æ¸¬è©¦"""
        start_time = time.time()
        try:
            result = engine.search(
                query=f"test_query_{query_id}",
                k=5,
                embedding_fn=mock_embedding,
                use_pq_search=True
            )
            search_time = time.time() - start_time
            return search_time
        except Exception as e:
            print(f"æœç´¢ {query_id} å¤±æ•—: {e}")
            return -1
    
    print(f"ğŸ” é–‹å§‹æ€§èƒ½æ¸¬è©¦ï¼š{num_queries} å€‹ä¸¦ç™¼æŸ¥è©¢")
    print(f"ğŸ“Š é›†åˆä¿¡æ¯ï¼š{collection_name}, ç¶­åº¦ï¼š{engine.info.dimension}")
    
    # å–®ç·šç¨‹æ¸¬è©¦ï¼ˆåŸºæº–ï¼‰
    print("\nğŸ“ˆ å–®ç·šç¨‹æ¸¬è©¦ï¼ˆåŸºæº–ï¼‰...")
    single_thread_times = []
    start_time = time.time()
    
    for i in range(num_queries):
        search_time = single_search(i)
        if search_time > 0:
            single_thread_times.append(search_time)
    
    single_thread_total = time.time() - start_time
    single_thread_avg = np.mean(single_thread_times) if single_thread_times else 0
    
    print(f"   ç¸½æ™‚é–“ï¼š{single_thread_total:.3f}s")
    print(f"   å¹³å‡æœç´¢æ™‚é–“ï¼š{single_thread_avg*1000:.2f}ms")
    print(f"   æˆåŠŸæŸ¥è©¢ï¼š{len(single_thread_times)}/{num_queries}")
    
    # å¤šç·šç¨‹æ¸¬è©¦
    print("\nğŸš€ å¤šç·šç¨‹æ¸¬è©¦ï¼ˆç·šç¨‹å®‰å…¨ï¼‰...")
    multi_thread_times = []
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        future_to_query = {
            executor.submit(single_search, i): i 
            for i in range(num_queries)
        }
        
        # æ”¶é›†çµæœ
        for future in as_completed(future_to_query):
            query_id = future_to_query[future]
            try:
                search_time = future.result()
                if search_time > 0:
                    multi_thread_times.append(search_time)
            except Exception as e:
                print(f"æŸ¥è©¢ {query_id} ç•°å¸¸ï¼š{e}")
    
    multi_thread_total = time.time() - start_time
    multi_thread_avg = np.mean(multi_thread_times) if multi_thread_times else 0
    
    print(f"   ç¸½æ™‚é–“ï¼š{multi_thread_total:.3f}s")
    print(f"   å¹³å‡æœç´¢æ™‚é–“ï¼š{multi_thread_avg*1000:.2f}ms")
    print(f"   æˆåŠŸæŸ¥è©¢ï¼š{len(multi_thread_times)}/{num_queries}")
    
    # æ€§èƒ½æ¯”è¼ƒ
    print("\nğŸ“Š æ€§èƒ½æ¯”è¼ƒçµæœï¼š")
    if single_thread_avg > 0 and multi_thread_avg > 0:
        speedup = single_thread_total / multi_thread_total
        overhead = (multi_thread_avg - single_thread_avg) / single_thread_avg * 100
        
        print(f"   ä¸¦ç™¼åŠ é€Ÿæ¯”ï¼š{speedup:.2f}x")
        print(f"   å–®æ¬¡æœç´¢é–‹éŠ·ï¼š{overhead:+.1f}%")
        
        if overhead < 5:
            print("   âœ… æ€§èƒ½æ¶ˆè€—å¯æ¥å—ï¼ˆ< 5%ï¼‰")
        elif overhead < 10:
            print("   âš ï¸  æ€§èƒ½æ¶ˆè€—ä¸­ç­‰ï¼ˆ5-10%ï¼‰")
        else:
            print("   âŒ æ€§èƒ½æ¶ˆè€—è¼ƒå¤§ï¼ˆ> 10%ï¼‰")
    
    # çµ±è¨ˆè³‡æ–™
    print("\nğŸ“ˆ çµ±è¨ˆè³‡æ–™ï¼š")
    stats = engine.get_search_statistics()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")
    
    return {
        'single_thread_avg': single_thread_avg,
        'multi_thread_avg': multi_thread_avg,
        'speedup': speedup if 'speedup' in locals() else 0,
        'overhead': overhead if 'overhead' in locals() else 0
    }