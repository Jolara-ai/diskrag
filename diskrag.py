#!/usr/bin/env python3
"""
DiskRAG - ç°¡åŒ–çš„ä¸»ç¨‹å¼å…¥å£
"""
import argparse
import logging
import sys
from pathlib import Path
import yaml
import numpy as np
import polars as pl
from typing import Optional, List, Dict, Any
import time
import os

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
try:
    from dotenv import load_dotenv
    # è¼‰å…¥ .env æ–‡ä»¶
    load_dotenv()
except ImportError:
    # å¦‚æœæ²’æœ‰å®‰è£ python-dotenvï¼Œå˜—è©¦æ‰‹å‹•è¼‰å…¥ .env æ–‡ä»¶
    env_file = Path('.env')
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# é è™•ç†ç›¸é—œ
from preprocessing.collection import CollectionManager
from preprocessing.config import load_config, PreprocessingConfig, EmbeddingConfig, QuestionGenerationConfig, ChunkConfig, OutputConfig
from preprocessing.processor import Preprocessor
from preprocessing.chunker import DocumentProcessor

# ç´¢å¼•ç›¸é—œ
from pydiskann.vamana_graph import build_vamana
from pydiskann.io.diskann_persist import DiskANNPersist

# æœå°‹ç›¸é—œ
from search_engine import SearchEngine
from openai import OpenAI

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DiskRAG:
    """çµ±ä¸€çš„ DiskRAG æ“ä½œä»‹é¢"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config_path = Path(config_path)
        if self.config_path.exists():
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.global_config = yaml.safe_load(f)
        else:
            self.global_config = {}
        self.manager = CollectionManager()
        
        # æª¢æŸ¥ OpenAI API Key
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError(
                "OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®ã€‚è«‹ï¼š\n"
                "1. åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½® OPENAI_API_KEY=your-api-key\n"
                "2. æˆ–è¨­ç½®ç’°å¢ƒè®Šæ•¸ï¼šexport OPENAI_API_KEY=your-api-key"
            )
        
        self.client = OpenAI(api_key=api_key)
        
    def process(self, input_path: str, collection: Optional[str] = None, 
                generate_questions: bool = False) -> None:
        """è™•ç†æª”æ¡ˆï¼ˆè‡ªå‹•åˆ¤æ–·é¡å‹ï¼‰"""
        input_file = Path(input_path)
        if not input_file.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {input_path}")
            
        # è¼‰å…¥æˆ–å»ºç«‹è¨­å®š
        if self.config_path.exists():
            config = load_config(str(self.config_path))
            if collection:
                config.collection = collection
        else:
            config = self._create_default_config(collection or "default_collection")

        if collection is None:
            collection = config.collection

        # æ ¹æ“šæª”æ¡ˆé¡å‹è™•ç†
        if input_file.suffix.lower() == '.csv':
            logger.info(f"è™•ç† CSV æª”æ¡ˆ: {input_file}")
            self._process_csv(input_file, config, generate_questions)
        elif input_file.suffix.lower() in ['.md', '.markdown']:
            logger.info(f"è™•ç† Markdown æª”æ¡ˆ: {input_file}")
            self._process_document(input_file, config, 'md')
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {input_file.suffix}ã€‚ç›®å‰æ”¯æ´: .csv, .md, .markdown")
            
    def _process_csv(self, input_file: Path, config: PreprocessingConfig, 
                     generate_questions: bool) -> None:
        """è™•ç† CSV æª”æ¡ˆ"""
        config.question_generation.enabled = generate_questions
        processor = Preprocessor(config)
        
        processor.process_file(str(input_file))
            
    def _process_document(self, input_file: Path, config: PreprocessingConfig, doc_type: str) -> None:
        """è™•ç† Markdown æª”æ¡ˆ"""
        processor = DocumentProcessor(
            collection_name=config.collection,
            manual_dir=str(input_file.parent),
            config_path=str(self.config_path)
        )
        chunks = processor.process_markdown(input_file)
        
        if chunks:
            self._save_chunks(chunks, processor, config, input_file)
            
    def _save_chunks(self, chunks: List[Any], processor: DocumentProcessor, 
                     config: PreprocessingConfig, input_file: Path = None) -> None:
        """å„²å­˜æ–‡å­—å¡Š"""
        texts = [chunk.text for chunk in chunks]
        metadata_list = [{
            "id": chunk.id,
            "text": chunk.text,
            "image": chunk.image,
            "section": chunk.section,
            "manual": chunk.manual,
            "source_type": "manual",
            "source_id": str(chunk.id),
            "is_question": False
        } for chunk in chunks]
        
        # ç”Ÿæˆå‘é‡
        logger.info(f"ç‚º {len(texts)} å€‹æ–‡å­—å¡Šç”Ÿæˆå‘é‡...")
        embedding_results, valid_indices = processor.embedding_generator.generate_embeddings(texts)
        
        if embedding_results:
            vectors = np.array([r.vector for r in embedding_results])
            valid_texts = [r.text for r in embedding_results]
            valid_metadata = [metadata_list[i] for i in valid_indices]
            
            # Ensure collection exists before updating
            info = self.manager.get_collection_info(config.collection)
            if not info:
                dim = vectors.shape[1]
                self.manager.create_collection(
                    collection_name=config.collection,
                    config=config.to_dict(),
                    dimension=dim,
                    source_files=[str(input_file.name) if input_file else "unknown"]
                )

            self.manager.update_collection(
                collection_name=config.collection,
                vectors=vectors,
                texts=valid_texts,
                metadata_list=valid_metadata
            )
            logger.info(f"æˆåŠŸè™•ç† {len(valid_texts)} å€‹æ–‡å­—å¡Š")
            
    
    def build_index(self, collection: str, target_quality: str = "balanced", 
                    verbose: bool = False, force_rebuild: bool = False) -> None:
        """
        ç‚º collection å»ºç«‹ç´¢å¼•
        """
        try:
            from scripts.tools.build_index import build_index as build_index_func
            
            build_index_func(
                collection_name=collection,
                target_quality=target_quality,
                verbose=verbose,
                force_rebuild=force_rebuild
            )
        except Exception as e:
            logger.error(f"ç‚º collection '{collection}' å»ºç«‹ç´¢å¼•æ™‚å¤±æ•—: {e}")
            raise
        
    def search(self, collection: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æœå°‹"""
        engine = SearchEngine(collection)
        
        # ç”ŸæˆæŸ¥è©¢å‘é‡
        response = self.client.embeddings.create(
            model=self.global_config.get('embedding', {}).get('model', 'text-embedding-3-small'),
            input=query
        )
        query_vector = np.array(response.data[0].embedding)
        
        # åŸ·è¡Œæœå°‹
        results = engine.search(
            query=query,
            k=top_k,
            embedding_fn=lambda x: query_vector
        )
        
        return results["results"]
        
    def list_collections(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰ collections"""
        collections = self.manager.list_collections()
        if not collections:
            print("æ²’æœ‰ä»»ä½• collection")
            return
            
        print("\nå¯ç”¨çš„ Collections:")
        print("-" * 60)
        for col in sorted(collections, key=lambda c: c.name):
            print(f"  - {col.name} (å‘é‡æ•¸: {col.num_vectors})")
        print("-" * 60)
            
    def delete_collection(self, collection: str) -> None:
        """åˆªé™¤ collection"""
        confirm = input(f"ç¢ºå®šè¦æ°¸ä¹…åˆªé™¤ collection '{collection}' åŠå…¶æ‰€æœ‰è³‡æ–™å—ï¼Ÿ(y/N): ")
        if confirm.lower() == 'y':
            self.manager.delete_collection(collection)
            logger.info(f"å·²åˆªé™¤ collection: {collection}")
        else:
            print("å–æ¶ˆåˆªé™¤ã€‚")

    def process_directory(self, directory: str, collection_prefix: str = None,
                         recursive: bool = False, pattern: str = "*") -> Dict[str, Any]:
        """è™•ç†ç›®éŒ„ä¸­çš„æ‰€æœ‰æ”¯æ´æª”æ¡ˆ"""
        dir_path = Path(directory)
        if not dir_path.exists():
            raise ValueError(f"ç›®éŒ„ä¸å­˜åœ¨: {directory}")
            
        supported_extensions = {'.csv', '.md', '.markdown'}
        
        # æ”¶é›†æª”æ¡ˆ
        if recursive:
            files = []
            for ext in supported_extensions:
                files.extend(dir_path.rglob(f"{pattern}{ext}"))
        else:
            files = []
            for ext in supported_extensions:
                files.extend(dir_path.glob(f"{pattern}{ext}"))
                
        if not files:
            logger.warning(f"åœ¨ {directory} ä¸­æ²’æœ‰æ‰¾åˆ°æ”¯æ´çš„æª”æ¡ˆ")
            return {"processed": 0, "failed": 0, "files": []}
            
        logger.info(f"æ‰¾åˆ° {len(files)} å€‹æª”æ¡ˆ")
        
        # è™•ç†æª”æ¡ˆ
        results = {
            "processed": 0,
            "failed": 0,
            "files": []
        }
        
        for file in sorted(files):
            # æ±ºå®š collection åç¨±
            if collection_prefix:
                collection = f"{collection_prefix}_{file.stem}"
            else:
                collection = file.stem
                
            logger.info(f"è™•ç† {file.name} -> collection: {collection}")
            
            try:
                # è™•ç†æª”æ¡ˆ
                self.process(str(file), collection)
                
                # å»ºç«‹ç´¢å¼•
                logger.info(f"å»ºç«‹ç´¢å¼•: {collection}")
                self.build_index(collection)
                
                results["processed"] += 1
                results["files"].append({
                    "file": str(file),
                    "collection": collection,
                    "status": "success"
                })
                
            except Exception as e:
                logger.error(f"è™•ç† {file} æ™‚å¤±æ•—: {str(e)}")
                results["failed"] += 1
                results["files"].append({
                    "file": str(file),
                    "collection": collection,
                    "status": "failed",
                    "error": str(e)
                })
                
        return results

    def merge_collections(self, collections: List[str], target_collection: str) -> None:
        """åˆä½µå¤šå€‹ collections åˆ°ä¸€å€‹"""
        logger.info(f"åˆä½µ {len(collections)} å€‹ collections åˆ° {target_collection}")
        
        all_vectors = []
        all_texts = []
        
        # æ”¶é›†æ‰€æœ‰æ•¸æ“š
        for collection in collections:
            info = self.manager.get_collection_info(collection)
            if not info:
                logger.warning(f"æ‰¾ä¸åˆ° collection: {collection}")
                continue
                
            vectors_path = self.manager.get_vectors_path(collection)
            metadata_path = self.manager.get_metadata_path(collection)
            
            if not vectors_path.exists() or not metadata_path.exists():
                logger.warning(f"collection {collection} çš„æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨")
                continue
                
            vectors = np.load(str(vectors_path))
            texts_df = pl.read_parquet(str(metadata_path))
            
            all_vectors.append(vectors)
            all_texts.append(texts_df)
            
            logger.info(f"å·²è¼‰å…¥ {collection}: {len(vectors)} å€‹å‘é‡")
        
        if not all_vectors:
            raise ValueError("æ²’æœ‰æœ‰æ•ˆçš„ collections å¯ä»¥åˆä½µ")
        
        # åˆä½µæ•¸æ“š
        merged_vectors = np.vstack(all_vectors)
        merged_texts = pl.concat(all_texts)
        
        # å‰µå»ºæ–°çš„ collection
        dimension = merged_vectors.shape[1]
        config = self._create_default_config(target_collection)
        
        self.manager.create_collection(
            collection_name=target_collection,
            config=config.to_dict(),
            dimension=dimension,
            source_files=collections
        )
        
        # ä¿å­˜åˆä½µçš„æ•¸æ“š
        self.manager.save_vectors(target_collection, merged_vectors)
        # ä¿å­˜åˆä½µçš„å…ƒæ•¸æ“š
        metadata_path = self.manager.get_metadata_path(target_collection)
        merged_texts.write_parquet(str(metadata_path))
        
        logger.info(f"æˆåŠŸåˆä½µåˆ° {target_collection}: {len(merged_vectors)} å€‹å‘é‡")

    def doctor_collection(self, collection: str) -> bool:
        """ä¿®å¾©æŒ‡å®šé›†åˆçš„ PQ æ¨¡å‹"""
        logger.info(f"ğŸ”§ é–‹å§‹ä¿®å¾©é›†åˆ '{collection}' çš„ PQ æ¨¡å‹...")
        
        try:
            info = self.manager.get_collection_info(collection)
            if not info:
                logger.error(f"âŒ æ‰¾ä¸åˆ°é›†åˆ: {collection}")
                return False
            
            # è¼‰å…¥åŸå§‹å‘é‡æ•¸æ“š
            vectors_path = self.manager.get_vectors_path(collection)
            
            if not vectors_path.exists():
                logger.error(f"âŒ å‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {vectors_path}")
                return False
                
            vectors = np.load(str(vectors_path))
            
            # æª¢æŸ¥å‘é‡æ•¸æ“šæ˜¯å¦ç‚ºç©ºæˆ–æå£
            if vectors.size == 0:
                logger.error(f"âŒ å‘é‡æ•¸æ“šç‚ºç©ºï¼æ–‡ä»¶: {vectors_path}")
                return False
            
            # æª¢æŸ¥å‘é‡æ•¸é‡æ˜¯å¦èˆ‡é›†åˆä¿¡æ¯ä¸€è‡´
            if info and len(vectors) != info.num_vectors:
                logger.warning(f"âš ï¸  å‘é‡æ•¸é‡ä¸åŒ¹é…: æ–‡ä»¶ä¸­æœ‰ {len(vectors)} å€‹ï¼Œé›†åˆä¿¡æ¯é¡¯ç¤º {info.num_vectors} å€‹")
                
                # å˜—è©¦å¾ç´¢å¼•æ–‡ä»¶ä¸­æ¢å¾©å‘é‡
                logger.info("ğŸ”§ å˜—è©¦å¾ç´¢å¼•æ–‡ä»¶ä¸­æ¢å¾©å‘é‡...")
                try:
                    from pydiskann.io.diskann_persist import MMapNodeReader
                    index_dir = self.manager.get_index_dir(collection)
                    reader = MMapNodeReader(str(index_dir / "index.dat"), dim=info.dimension)
                    
                    # è®€å–æ‰€æœ‰å‘é‡
                    recovered_vectors = []
                    for i in range(info.num_vectors):
                        vec, _ = reader.get_node(i)
                        recovered_vectors.append(vec)
                    
                    vectors = np.array(recovered_vectors, dtype=np.float32)
                    reader.close()
                    
                    logger.info(f"âœ… æˆåŠŸå¾ç´¢å¼•æ¢å¾© {len(vectors)} å€‹å‘é‡")
                    
                    # ä¿å­˜æ¢å¾©çš„å‘é‡
                    np.save(str(vectors_path), vectors)
                    logger.info(f"ğŸ’¾ å·²ä¿å­˜æ¢å¾©çš„å‘é‡åˆ°: {vectors_path}")
                    
                except Exception as e:
                    logger.error(f"âŒ ç„¡æ³•å¾ç´¢å¼•æ¢å¾©å‘é‡: {e}")
                    return False
            
            # ç¢ºä¿æ•¸æ“šé¡å‹ç‚º float32
            if vectors.dtype != np.float32:
                logger.info(f"ğŸ”„ è½‰æ›æ•¸æ“šé¡å‹å¾ {vectors.dtype} åˆ° float32")
                vectors = vectors.astype(np.float32)
            
            logger.info(f"ğŸ“Š å‘é‡æ•¸æ“šçµ±è¨ˆ:")
            logger.info(f"  - å½¢ç‹€: {vectors.shape}")
            logger.info(f"  - æ•¸æ“šé¡å‹: {vectors.dtype}")
            logger.info(f"  - å‘é‡æ•¸é‡: {len(vectors)}")
            
            # é‡æ–°è¨“ç·´ PQ æ¨¡å‹
            logger.info("ğŸ”„ é‡æ–°è¨“ç·´ PQ æ¨¡å‹...")
            from pydiskann.pq.fast_pq import DiskANNPQ
            
            pq = DiskANNPQ(
                dimension=info.dimension,
                num_subvectors=info.pq_config.get('num_subvectors', 8),
                num_centroids=info.pq_config.get('num_centroids', 256)
            )
            
            pq.train(vectors)
            
            # ä¿å­˜æ–°çš„ PQ æ¨¡å‹
            pq_path = self.manager.get_pq_path(collection)
            pq.save(str(pq_path))
            
            logger.info(f"âœ… PQ æ¨¡å‹ä¿®å¾©å®Œæˆï¼å·²ä¿å­˜åˆ°: {pq_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿®å¾© PQ æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
            
    def _create_default_config(self, collection: str) -> PreprocessingConfig:
        """å»ºç«‹é è¨­è¨­å®š"""
        return PreprocessingConfig(
            collection=collection,
            embedding=EmbeddingConfig(provider="openai", model="text-embedding-3-small"),
            question_generation=QuestionGenerationConfig(enabled=False, provider="openai", model="gpt-4o-mini"),
            chunk=ChunkConfig(),
            output=OutputConfig()
        )

def main():
    parser = argparse.ArgumentParser(
        description='DiskRAG - ä¸€å€‹åŸºæ–¼ç£ç¢Ÿçš„ RAG ç³»çµ± CLI å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤', required=True)

    # --- Process Command ---
    process_parser = subparsers.add_parser('process', help='è™•ç†ä¾†æºæª”æ¡ˆä¸¦ç”Ÿæˆå‘é‡')
    process_parser.add_argument('file', help='è¦è™•ç†çš„æª”æ¡ˆè·¯å¾‘ (.csv, .md, .markdown)')
    process_parser.add_argument('--collection', '-c', help='æŒ‡å®š collection åç¨± (é è¨­: å¾æª”åæˆ–è¨­å®šæª”ä¸­ç²å–)')
    process_parser.add_argument('--questions', '-q', action='store_true', help='ç‚º FAQ (CSV) ç”Ÿæˆç›¸ä¼¼å•é¡Œ')

    # --- Index Command ---
    index_parser = subparsers.add_parser('index', help='ç‚º collection å»ºç«‹ç´¢å¼•')
    index_parser.add_argument('collection', help='è¦å»ºç«‹ç´¢å¼•çš„ collection åç¨±')
    index_parser.add_argument('--target-quality', choices=['fast', 'balanced', 'high'], 
                             default='balanced', help='ç›®æ¨™å“è³ªç´šåˆ¥ (é è¨­: balanced)')
    index_parser.add_argument('--force-rebuild', action='store_true', 
                             help='å¼·åˆ¶é‡å»ºç´¢å¼•ï¼ˆå¿½ç•¥å·²å­˜åœ¨çš„ç´¢å¼•ï¼‰')

    # --- Search Command ---
    search_parser = subparsers.add_parser('search', help='åœ¨ collection ä¸­æœå°‹')
    search_parser.add_argument('collection', help='è¦æœå°‹çš„ collection åç¨±')
    search_parser.add_argument('query', help='æœå°‹çš„æŸ¥è©¢èªå¥')
    search_parser.add_argument('--top-k', '-k', type=int, default=5, help='å›å‚³çµæœæ•¸é‡ (é è¨­: 5)')

    # --- Process Directory Command ---
    process_dir_parser = subparsers.add_parser('process-dir', help='è™•ç†æ•´å€‹ç›®éŒ„çš„æª”æ¡ˆ')
    process_dir_parser.add_argument('directory', help='è¦è™•ç†çš„ç›®éŒ„è·¯å¾‘')
    process_dir_parser.add_argument('--prefix', '-p', help='collection åç¨±å‰ç¶´')
    process_dir_parser.add_argument('--recursive', '-r', action='store_true', help='éè¿´è™•ç†å­ç›®éŒ„')
    process_dir_parser.add_argument('--pattern', default='*', help='æª”æ¡ˆåŒ¹é…æ¨¡å¼ (é è¨­: *)')

    # --- Merge Collections Command ---
    merge_parser = subparsers.add_parser('merge', help='åˆä½µå¤šå€‹ collections')
    merge_parser.add_argument('collections', nargs='+', help='è¦åˆä½µçš„ collection åç¨±')
    merge_parser.add_argument('--target', '-t', required=True, help='ç›®æ¨™ collection åç¨±')

    # --- Doctor Command ---
    doctor_parser = subparsers.add_parser('doctor', help='ä¿®å¾© collection çš„ PQ æ¨¡å‹')
    doctor_parser.add_argument('collection', help='è¦ä¿®å¾©çš„ collection åç¨±')

    # --- Manage Commands ---
    subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰ collections')
    delete_parser = subparsers.add_parser('delete', help='åˆªé™¤ä¸€å€‹ collection')
    delete_parser.add_argument('collection', help='è¦åˆªé™¤çš„ collection åç¨±')

  
    parser.add_argument('--config', default='config.yaml', help='è¨­å®šæª”è·¯å¾‘ (é è¨­: config.yaml)')
    parser.add_argument('--verbose', '-v', action='store_true', help='é¡¯ç¤ºè©³ç´°æ—¥èªŒ')

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.INFO)

    rag = DiskRAG(args.config)

    try:
        if args.command == 'process':
            rag.process(args.file, args.collection, args.questions)
            print(f"\nè™•ç†å®Œæˆï¼è«‹è¨˜å¾—åŸ·è¡Œ 'python diskrag.py index {args.collection or Path(args.file).stem}' ä¾†å»ºç«‹ç´¢å¼•ã€‚")

        elif args.command == 'index':
            rag.build_index(
                args.collection, 
                target_quality=getattr(args, 'target_quality', 'balanced'),
                verbose=args.verbose,
                force_rebuild=args.force_rebuild
            )

        elif args.command == 'search':
            results = rag.search(args.collection, args.query, args.top_k)
            print(f"\næœå°‹ \"{args.query}\" çš„çµæœ (å…± {len(results)} ç­†):")
            print("-" * 80)
            for i, result in enumerate(results, 1):
                similarity = 1 - result['distance'] # å‡è¨­è·é›¢æ˜¯ 0-2 ä¹‹é–“
                print(f"[{i}] ç›¸ä¼¼åº¦: {similarity:.2%}")
                
                # æª¢æŸ¥æ˜¯å¦ç‚º FAQ é¡å‹ï¼Œå¦‚æœæ˜¯å‰‡é¡¯ç¤ºç­”æ¡ˆ
                metadata = result.get('metadata', {})
                if isinstance(metadata, str):
                    try:
                        import json
                        metadata = json.loads(metadata)
                    except:
                        metadata = {}
                
                # æª¢æŸ¥æ˜¯å¦ç‚º FAQï¼ˆæ”¯æ´åµŒå¥— metadataï¼‰
                is_faq = False
                answer = None
                question = None
                
                # æ–¹æ³•1: æª¢æŸ¥é ‚å±¤æ˜¯å¦æœ‰ answer æ¬„ä½ï¼ˆFAQ çš„æ¨™èªŒï¼‰
                if metadata.get('answer'):
                    is_faq = True
                    answer = metadata.get('answer')
                    question = metadata.get('original_question') or result.get('text', '')
                # æ–¹æ³•2: æª¢æŸ¥é ‚å±¤çš„ type
                elif metadata.get('type') == 'faq':
                    is_faq = True
                    answer = metadata.get('answer')
                    question = metadata.get('original_question') or result.get('text', '')
                # æ–¹æ³•3: æª¢æŸ¥æ˜¯å¦æœ‰åµŒå¥—çš„ metadataï¼ˆå¾ parquet è®€å–æ™‚å¯èƒ½æ˜¯å­—ä¸²ï¼‰
                else:
                    nested_meta_str = metadata.get('metadata')
                    if nested_meta_str:
                        if isinstance(nested_meta_str, str):
                            try:
                                nested_meta = json.loads(nested_meta_str)
                            except:
                                nested_meta = {}
                        else:
                            nested_meta = nested_meta_str
                        
                        if isinstance(nested_meta, dict) and nested_meta.get('type') == 'faq':
                            is_faq = True
                            # ç­”æ¡ˆåœ¨é ‚å±¤ï¼Œå•é¡Œå¯èƒ½åœ¨é ‚å±¤æˆ–åµŒå¥—å±¤
                            answer = metadata.get('answer')
                            question = metadata.get('original_question') or nested_meta.get('original_question') or result.get('text', '')
                
                if is_faq and answer:
                    # FAQ æœå°‹çµæœï¼šé¡¯ç¤ºç­”æ¡ˆ
                    print(f"    å•é¡Œ: {question}")
                    print(f"    ç­”æ¡ˆ: {answer}")
                else:
                    # ä¸€èˆ¬æœå°‹çµæœï¼šé¡¯ç¤ºæ–‡å­—å…§å®¹
                    text_content = result['text'].strip()
                    print(f"    å…§å®¹: {text_content}")
                
                # é¡¯ç¤ºä¾†æºè³‡è¨Š
                if metadata:
                    source = metadata.get('manual') or metadata.get('source_type') or metadata.get('source_file')
                    if source:
                        print(f"    ä¾†æº: {source}")
            print("-" * 80)

        elif args.command == 'list':
            rag.list_collections()

        elif args.command == 'process-dir':
            results = rag.process_directory(
                args.directory,
                collection_prefix=args.prefix,
                recursive=args.recursive,
                pattern=args.pattern
            )
            print(f"\nç›®éŒ„è™•ç†å®Œæˆï¼")
            print(f"æˆåŠŸè™•ç†: {results['processed']} å€‹æª”æ¡ˆ")
            print(f"è™•ç†å¤±æ•—: {results['failed']} å€‹æª”æ¡ˆ")
            if results['files']:
                print("\nè©³ç´°çµæœ:")
                for file_info in results['files']:
                    status = "âœ…" if file_info['status'] == 'success' else "âŒ"
                    print(f"  {status} {file_info['file']} -> {file_info['collection']}")
                    if file_info['status'] == 'failed':
                        print(f"    éŒ¯èª¤: {file_info['error']}")

        elif args.command == 'merge':
            rag.merge_collections(args.collections, args.target)
            print(f"\nåˆä½µå®Œæˆï¼è«‹åŸ·è¡Œ 'python diskrag.py index {args.target}' ä¾†å»ºç«‹ç´¢å¼•ã€‚")

        elif args.command == 'doctor':
            success = rag.doctor_collection(args.collection)
            if success:
                print(f"\nâœ… ä¿®å¾©å®Œæˆï¼collection '{args.collection}' çš„ PQ æ¨¡å‹å·²ä¿®å¾©ã€‚")
            else:
                print(f"\nâŒ ä¿®å¾©å¤±æ•—ï¼è«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯ã€‚")

        elif args.command == 'delete':
            rag.delete_collection(args.collection)


    except Exception as e:
        logger.error(f"åŸ·è¡Œå‘½ä»¤ '{args.command}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        # if args.verbose:
        #     import traceback
        #     traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()