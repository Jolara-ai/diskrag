import numpy as np
import json
import pickle
import mmap
import logging
from collections import OrderedDict
from pathlib import Path

logger = logging.getLogger(__name__)

class DiskANNPersist:
    def __init__(self, dim=128, R=16, record_bytes=None):
        self.D = dim
        self.R = R
        self.record_size = 4 * (dim + R) if record_bytes is None else record_bytes

    def save_index(self, filepath, graph):
        with open(filepath, 'wb') as f:
            for idx in range(len(graph.nodes)):
                vec = graph.nodes[idx].vector.astype(np.float32)
                f.write(vec.tobytes())
                neighbors = list(graph.nodes[idx].neighbors)
                neighbors += [0] * (self.R - len(neighbors))
                f.write(np.array(neighbors[:self.R], dtype=np.uint32).tobytes())

    def save_meta(self, filepath, meta_dict):
        with open(filepath, 'w') as f:
            json.dump(meta_dict, f)

    def save_pq_codes(self, filepath, pq_codes):
        pq_codes.astype(np.uint8).tofile(filepath)

    def save_pq_codebook(self, filepath, pq_model):
        """æ”¹é€²çš„ PQ æ¨¡åž‹ä¿å­˜æ–¹æ³• - è§£æ±ºåºåˆ—åŒ–å•é¡Œ"""
        logger.info(f"ðŸ”§ é–‹å§‹ä¿å­˜ PQ æ¨¡åž‹åˆ°: {filepath}")
        
        # æª¢æŸ¥æ¨¡åž‹å®Œæ•´æ€§
        if not hasattr(pq_model, 'is_fitted') or not pq_model.is_fitted:
            raise ValueError("PQ æ¨¡åž‹æœªè¨“ç·´å®Œæˆï¼Œç„¡æ³•ä¿å­˜")
        
        if not hasattr(pq_model, 'kmeans_list') or not pq_model.kmeans_list:
            raise ValueError("PQ æ¨¡åž‹ç¼ºå°‘ kmeans_listï¼Œç„¡æ³•ä¿å­˜")
        
        # å‰µå»ºåŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯çš„å­—å…¸
        model_data = {
            'n_subvectors': pq_model.n_subvectors,
            'n_centroids': pq_model.n_centroids,
            'sub_dim': pq_model.sub_dim,
            'is_fitted': pq_model.is_fitted,
            'kmeans_list': pq_model.kmeans_list,
            'means_': getattr(pq_model, 'means_', None),
            'stds_': getattr(pq_model, 'stds_', None),
            'epsilon': getattr(pq_model, 'epsilon', 1e-8),
            'model_type': 'DiskANNPQ',
            'version': '2.0'
        }
        
        # é©—è­‰é—œéµæ•¸æ“šå®Œæ•´æ€§
        logger.info("ðŸ” é©—è­‰ PQ æ¨¡åž‹æ•¸æ“šå®Œæ•´æ€§...")
        logger.info(f"  - n_subvectors: {model_data['n_subvectors']}")
        logger.info(f"  - n_centroids: {model_data['n_centroids']}")
        logger.info(f"  - sub_dim: {model_data['sub_dim']}")
        logger.info(f"  - is_fitted: {model_data['is_fitted']}")
        logger.info(f"  - kmeans_list é•·åº¦: {len(model_data['kmeans_list'])}")
        logger.info(f"  - means_ å­˜åœ¨: {model_data['means_'] is not None}")
        logger.info(f"  - stds_ å­˜åœ¨: {model_data['stds_'] is not None}")
        
        # æª¢æŸ¥æ¯å€‹ KMeans æ¨¡åž‹
        for i, kmeans in enumerate(model_data['kmeans_list']):
            if not hasattr(kmeans, 'cluster_centers_'):
                raise ValueError(f"KMeans æ¨¡åž‹ {i} ç¼ºå°‘ cluster_centers_")
            centers_shape = kmeans.cluster_centers_.shape
            expected_shape = (model_data['n_centroids'], model_data['sub_dim'])
            if centers_shape != expected_shape:
                raise ValueError(f"KMeans æ¨¡åž‹ {i} èšé¡žä¸­å¿ƒå½¢ç‹€éŒ¯èª¤: {centers_shape}, é æœŸ: {expected_shape}")
        
        logger.info("âœ… PQ æ¨¡åž‹æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥é€šéŽ")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        try:
            # å…ˆä¿å­˜åˆ°è‡¨æ™‚æ–‡ä»¶
            temp_filepath = str(filepath) + '.tmp'
            with open(temp_filepath, 'wb') as f:
                pickle.dump(model_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            # é©—è­‰ä¿å­˜çš„æ–‡ä»¶å¯ä»¥æ­£ç¢ºåŠ è¼‰
            with open(temp_filepath, 'rb') as f:
                test_data = pickle.load(f)
            
            # åŸºæœ¬é©—è­‰
            assert test_data['model_type'] == 'DiskANNPQ'
            assert test_data['n_subvectors'] == model_data['n_subvectors']
            assert len(test_data['kmeans_list']) == len(model_data['kmeans_list'])
            
            # å¦‚æžœé©—è­‰é€šéŽï¼Œé‡å‘½åç‚ºæœ€çµ‚æ–‡ä»¶
            Path(temp_filepath).rename(filepath)
            logger.info(f"âœ… PQ æ¨¡åž‹å·²æˆåŠŸä¿å­˜è‡³: {filepath}")
            
        except Exception as e:
            logger.error(f"âŒ PQ æ¨¡åž‹ä¿å­˜å¤±æ•—: {e}")
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            temp_path = Path(str(filepath) + '.tmp')
            if temp_path.exists():
                temp_path.unlink()
            raise

    def load_pq_codebook(self, filepath):
        """æ”¹é€²çš„ PQ æ¨¡åž‹åŠ è¼‰æ–¹æ³• - è§£æ±ºååºåˆ—åŒ–å•é¡Œ"""
        logger.info(f"ðŸ”§ é–‹å§‹åŠ è¼‰ PQ æ¨¡åž‹å¾ž: {filepath}")
        
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ–°æ ¼å¼
            if isinstance(model_data, dict) and 'model_type' in model_data:
                logger.info("âœ… æª¢æ¸¬åˆ°æ–°æ ¼å¼ PQ æ¨¡åž‹")
                return self._load_new_format_pq(model_data)
            else:
                logger.warning(f"âš ï¸  æª¢æ¸¬åˆ°èˆŠæ ¼å¼ PQ æ¨¡åž‹ï¼Œå˜—è©¦å…¼å®¹åŠ è¼‰...")
                return self._load_legacy_format_pq(model_data)
                
        except Exception as e:
            logger.error(f"âŒ PQ æ¨¡åž‹åŠ è¼‰å¤±æ•—: {e}")
            raise
    
    def _load_new_format_pq(self, model_data):
        """åŠ è¼‰æ–°æ ¼å¼çš„ PQ æ¨¡åž‹"""
        from pydiskann.pq.fast_pq import DiskANNPQ
        
        # é©—è­‰æ•¸æ“šå®Œæ•´æ€§
        required_keys = ['n_subvectors', 'n_centroids', 'sub_dim', 'is_fitted', 'kmeans_list']
        for key in required_keys:
            if key not in model_data:
                raise ValueError(f"PQ æ¨¡åž‹æ•¸æ“šç¼ºå°‘å¿…è¦å­—æ®µ: {key}")
        
        logger.info("ðŸ” é‡å»º PQ æ¨¡åž‹...")
        logger.info(f"  - n_subvectors: {model_data['n_subvectors']}")
        logger.info(f"  - n_centroids: {model_data['n_centroids']}")
        logger.info(f"  - sub_dim: {model_data['sub_dim']}")
        logger.info(f"  - kmeans_list é•·åº¦: {len(model_data['kmeans_list'])}")
        
        # é‡å»º PQ æ¨¡åž‹
        pq_model = DiskANNPQ(
            n_subvectors=model_data['n_subvectors'],
            n_centroids=model_data['n_centroids']
        )
        
        # æ¢å¾©æ‰€æœ‰å±¬æ€§
        pq_model.sub_dim = model_data['sub_dim']
        pq_model.is_fitted = model_data['is_fitted']
        pq_model.kmeans_list = model_data['kmeans_list']
        pq_model.means_ = model_data.get('means_')
        pq_model.stds_ = model_data.get('stds_')
        pq_model.epsilon = model_data.get('epsilon', 1e-8)
        
        # é©—è­‰åŠ è¼‰çš„æ¨¡åž‹
        if not pq_model.kmeans_list:
            raise ValueError("åŠ è¼‰çš„ PQ æ¨¡åž‹ç¼ºå°‘ kmeans_list")
        
        if len(pq_model.kmeans_list) != pq_model.n_subvectors:
            raise ValueError(f"KMeans æ¨¡åž‹æ•¸é‡ä¸åŒ¹é…: {len(pq_model.kmeans_list)} != {pq_model.n_subvectors}")
        
        # æª¢æŸ¥æ¨™æº–åŒ–åƒæ•¸
        if pq_model.means_ is not None and pq_model.stds_ is not None:
            logger.info("âœ… åŒ…å«æ¨™æº–åŒ–åƒæ•¸ (means_, stds_)")
            expected_dim = pq_model.n_subvectors * pq_model.sub_dim
            if len(pq_model.means_) != expected_dim:
                raise ValueError(f"means_ ç¶­åº¦éŒ¯èª¤: {len(pq_model.means_)} != {expected_dim}")
            if len(pq_model.stds_) != expected_dim:
                raise ValueError(f"stds_ ç¶­åº¦éŒ¯èª¤: {len(pq_model.stds_)} != {expected_dim}")
        else:
            logger.warning("âš ï¸  ç¼ºå°‘æ¨™æº–åŒ–åƒæ•¸ï¼Œå¯èƒ½å½±éŸ¿æœç´¢ç²¾åº¦")
        
        # æª¢æŸ¥æ¯å€‹ KMeans æ¨¡åž‹
        for i, kmeans in enumerate(pq_model.kmeans_list):
            if not hasattr(kmeans, 'cluster_centers_'):
                raise ValueError(f"KMeans æ¨¡åž‹ {i} ç¼ºå°‘ cluster_centers_")
            centers_shape = kmeans.cluster_centers_.shape
            expected_shape = (pq_model.n_centroids, pq_model.sub_dim)
            if centers_shape != expected_shape:
                raise ValueError(f"KMeans æ¨¡åž‹ {i} èšé¡žä¸­å¿ƒå½¢ç‹€éŒ¯èª¤: {centers_shape}, é æœŸ: {expected_shape}")
        
        logger.info("âœ… æ–°æ ¼å¼ PQ æ¨¡åž‹åŠ è¼‰æˆåŠŸ")
        return pq_model
    
    def _load_legacy_format_pq(self, model_data):
        """åŠ è¼‰èˆŠæ ¼å¼çš„ PQ æ¨¡åž‹ï¼ˆå‘å¾Œå…¼å®¹ï¼‰"""
        logger.warning("âš ï¸  æ­£åœ¨åŠ è¼‰èˆŠæ ¼å¼ PQ æ¨¡åž‹ï¼Œå»ºè­°é‡æ–°è¨“ç·´ä»¥ç²å¾—æœ€ä½³æ€§èƒ½")
        
        # æª¢æŸ¥èˆŠæ ¼å¼æ¨¡åž‹çš„åŸºæœ¬å®Œæ•´æ€§
        if hasattr(model_data, 'is_fitted') and model_data.is_fitted:
            if hasattr(model_data, 'kmeans_list') and model_data.kmeans_list:
                logger.info("âœ… èˆŠæ ¼å¼ PQ æ¨¡åž‹åŸºæœ¬å®Œæ•´æ€§æª¢æŸ¥é€šéŽ")
                return model_data
            else:
                raise ValueError("èˆŠæ ¼å¼ PQ æ¨¡åž‹ç¼ºå°‘ kmeans_list")
        else:
            raise ValueError("èˆŠæ ¼å¼ PQ æ¨¡åž‹æœªè¨“ç·´æˆ–ç¼ºå°‘ is_fitted æ¨™è¨˜")

    def load_meta(self, filepath):
        with open(filepath, 'r') as f:
            return json.load(f)

    def load_pq_codes(self, filepath, num_nodes, n_subvectors):
        return np.fromfile(filepath, dtype=np.uint8).reshape((num_nodes, n_subvectors))


class MMapNodeReader:
    def __init__(self, filepath, dim=128, R=16, cache_size=1024):
        self.D = dim
        self.R = R
        self.record_size = 4 * (dim + R)
        self.file = open(filepath, 'rb')
        self.mmap_obj = mmap.mmap(self.file.fileno(), 0, access=mmap.ACCESS_READ)
        self.cache = OrderedDict()
        self.cache_size = cache_size

    def get_node(self, node_id):
        if node_id in self.cache:
            self.cache.move_to_end(node_id)
            return self.cache[node_id]
        offset = node_id * self.record_size
        self.mmap_obj.seek(offset)
        vec = np.frombuffer(self.mmap_obj.read(4 * self.D), dtype=np.float32)
        neighbors = np.frombuffer(self.mmap_obj.read(4 * self.R), dtype=np.uint32)
        if len(self.cache) >= self.cache_size:
            self.cache.popitem(last=False)
        self.cache[node_id] = (vec, neighbors)
        return vec, neighbors

    def close(self):
        self.mmap_obj.close()
        self.file.close()
